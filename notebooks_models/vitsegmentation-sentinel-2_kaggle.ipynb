{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91683,"databundleVersionId":11007828,"sourceType":"competition"}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport shutil\nimport os\nimport re\n\ndef generate_label_mapping(root_dir, other_dir, input_subdir, output_csv):\n    \"\"\"\n    Generate a CSV mapping input chips to corresponding segmentation maps.\ns2_train/s2_train//chips/chip_20170701_S2A_MSIL2A_T42RWQ_20170526T054641_2_20.tif'\n    Args:\n        root_dir (str or Path): Root directory containing the subdirectories for chips and segmentation maps.\n        input_subdir (str): Subdirectory path for chips within the root directory.\n        output_csv (str or Path): Output path for the generated CSV file.\n    \"\"\"\n    root_dir = Path(root_dir)\n    chips_orig = os.listdir(root_dir / input_subdir / \"chips\")\n\n    chips = [chip.replace(\"chip\", f\"{input_subdir}/chips/chip\") for chip in chips_orig]\n    seg_maps = [chip.replace(\"chip\", f\"{input_subdir}/seg_maps/seg_map\") for chip in chips_orig]\n\n    df = pd.DataFrame({\"Input\": chips, \"Label\": seg_maps})\n    df.to_csv(other_dir + '/' + output_csv, index=False)\n    \n    print(f\"Number of rows is: {df.shape[0]}\")\n    print(f\"CSV generated and saved to: {root_dir / output_csv}\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T08:26:58.743446Z","iopub.execute_input":"2025-02-13T08:26:58.743719Z","iopub.status.idle":"2025-02-13T08:26:59.714860Z","shell.execute_reply.started":"2025-02-13T08:26:58.743688Z","shell.execute_reply":"2025-02-13T08:26:59.713943Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_label_mapping('/kaggle/input/geo-ai-hack/', '/kaggle/working/',\"s2_train/s2_train\", \"s2_train_ds.csv\")\ngenerate_label_mapping('/kaggle/input/geo-ai-hack/', '/kaggle/working/', \"s2_test/s2_test\", \"s2_test_ds.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T08:26:59.715685Z","iopub.execute_input":"2025-02-13T08:26:59.716013Z","iopub.status.idle":"2025-02-13T08:27:00.522149Z","shell.execute_reply.started":"2025-02-13T08:26:59.715991Z","shell.execute_reply":"2025-02-13T08:27:00.521293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install rasterio\nimport os\nimport torch\nimport numpy as np\nimport rasterio\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom transformers import ViTModel, ViTConfig\nimport torch.optim as optim\nimport torch.nn as nn\nimport pandas as pd\nfrom tqdm import tqdm\n\n# Custom Dataset Class with 3 Time-Step Stacking\nclass CustomSegmentationDataset(Dataset):\n    def __init__(self, image_paths, mask_paths=None, transform=None, is_test=False):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths if not is_test else None\n        self.transform = transform\n        self.is_test = is_test\n        self.samples = []\n        \n        # üî• Ne garder que les x premiers √©chantillons\n        self.samples = self.samples[:10]  # For debugging purposes\n\n        if not is_test:\n            for i in range(len(image_paths) - 2):  # Ensure 3 consecutive images exist\n                if os.path.exists(image_paths[i]) and os.path.exists(image_paths[i+1]) and os.path.exists(image_paths[i+2]) and os.path.exists(mask_paths[i]):\n                    self.samples.append((image_paths[i], image_paths[i+1], image_paths[i+2], mask_paths[i]))\n                else:\n                    print(f\"Missing files for index {i}: {[image_paths[i], image_paths[i+1], image_paths[i+2], mask_paths[i]]}\")\n        else:\n            for i in range(len(image_paths) - 2):\n                if os.path.exists(image_paths[i]) and os.path.exists(image_paths[i+1]) and os.path.exists(image_paths[i+2]):\n                    self.samples.append((image_paths[i], image_paths[i+1], image_paths[i+2], None))\n                else:\n                    print(f\"Missing files for index {i}: {[image_paths[i], image_paths[i+1], image_paths[i+2]]}\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        img1_path, img2_path, img3_path, mask_path = self.samples[idx]\n        preprocessing = Preprocessing()\n        img1 = preprocessing.preprocess_image(img1_path)\n        img2 = preprocessing.preprocess_image(img2_path)\n        img3 = preprocessing.preprocess_image(img3_path)\n        \n        # Stack images as additional channels (8*3 = 24 channels total)\n        image = np.concatenate([img1, img2, img3], axis=-1)\n        image = torch.tensor(image.transpose(2, 0, 1), dtype=torch.float32)\n\n        if self.is_test:\n            return image, {\"image_id\": torch.tensor([idx])}\n        else:\n            mask = plt.imread(mask_path)\n            if mask.ndim == 3:\n                mask = mask[..., 0]\n            mask = torch.tensor(mask, dtype=torch.long)\n            return image, mask\n\n# Preprocessing Class for 8-Channel Images\nimport cv2  # OpenCV for resizing\n\nclass Preprocessing:\n    def preprocess_image(self, image_path):\n        with rasterio.open(image_path) as src:\n            bands = [src.read(i) for i in range(1, 7)]\n        \n        ndvi = self.compute_ndvi(bands[2], bands[3])\n        evi = self.compute_evi(bands[3], bands[2], bands[0])\n        ndwi = self.compute_ndwi(bands[1], bands[3])\n        nbr = self.compute_nbr(bands[3], bands[5])\n        normalized_bands = [self.normalize_band(band) for band in bands]\n        image = np.stack(normalized_bands + [ndvi, evi, ndwi, nbr], axis=-1)  # Shape: (H, W, 8)\n\n        # üî• Resize to 256x256\n        image = cv2.resize(image, (256, 256), interpolation=cv2.INTER_NEAREST)\n        \n        return image\n\n    def normalize_band(self, band):\n        min_val, max_val = np.min(band), np.max(band)\n        return (band - min_val) / (max_val - min_val + 1e-6)\n\n    def compute_ndvi(self, red, nir):\n        return (nir - red) / (nir + red + 1e-6)\n\n    def compute_evi(self, nir, red, blue, g=2.5, c1=6, c2=7.5, l=1, epsilon=1e-6):\n        denominator = nir + c1 * red - c2 * blue + l\n        denominator = np.where(denominator == 0, epsilon, denominator)  # Avoid zero division\n        return np.clip(g * (nir - red) / denominator, 0, 1)\n        \n # TODO: add ndwi and change for 10 bands       \n    def compute_ndwi(self, green, nir, epsilon=1e-6):\n        denominator = green + nir\n        denominator = np.where(denominator == 0, epsilon, denominator)  # Avoid zero division\n        return (green - nir) / denominator\n    \n    def compute_nbr(self, nir, swir2, epsilon=1e-6):\n        denominator = nir + swir2\n        denominator = np.where(denominator == 0, epsilon, denominator)  # Avoid zero division\n        return (nir - swir2) / denominator\n\n\n# Vision Transformer for Segmentation (Updated for 30 channels) # 24\nimport torch.nn.functional as F\n\nclass ViTSegmentationModel(nn.Module):\n    def __init__(self, num_classes=2):\n        super(ViTSegmentationModel, self).__init__()\n\n        # ViT configuration with 30 channels # 24\n        config = ViTConfig(\n            image_size=256,        \n            patch_size=16,         \n            num_channels=30,      # 24 \n            hidden_size=384,       \n            num_attention_heads=6, \n            num_hidden_layers=12,  \n            intermediate_size=768, \n            num_classes=num_classes\n        )\n\n        self.vit = ViTModel(config)\n\n        # Segmentation decoder head\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(384, 128, kernel_size=2, stride=2),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, num_classes, kernel_size=2, stride=2)\n        )\n\n        # Final upsampling layer to match 256√ó256\n        self.upsample = nn.Upsample(size=(256, 256), mode='bilinear', align_corners=False)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        features = self.vit(x).last_hidden_state  # (batch_size, 257, hidden_size)\n\n        # Remove CLS token\n        features = features[:, 1:, :]  # Shape: (batch_size, 256, hidden_size)\n\n        # Reshape for decoder\n        spatial_size = int(features.shape[1] ** 0.5)  # Should be 16\n        features = features.permute(0, 2, 1).reshape(batch_size, 384, spatial_size, spatial_size)  # (batch, 384, 16, 16)\n\n        output = self.decoder(features)  # (batch, num_classes, 64, 64)\n\n        # Upsample to 256√ó256\n        output = self.upsample(output)  # (batch, num_classes, 256, 256)\n\n        return output\n\n# Update the input image size and patch size based on your dataset dimensions\n\n# Training Setup\ntrain_csv = pd.read_csv(\"/kaggle/working/s2_train_ds.csv\")\ntest_csv = pd.read_csv(\"/kaggle/working/s2_test_ds.csv\")\ntrain_image_paths = train_csv[\"Input\"].tolist()\ntrain_mask_paths = train_csv[\"Label\"].tolist()\ntest_image_paths = test_csv[\"Input\"].tolist()\n\n%cd /kaggle/input/geo-ai-hack\n\ntrain_dataset = CustomSegmentationDataset(image_paths=train_image_paths, mask_paths=train_mask_paths)\ntest_dataset = CustomSegmentationDataset(image_paths=test_image_paths, is_test=True)\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n\n# Model Training\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ViTSegmentationModel().to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\ncriterion = nn.CrossEntropyLoss()\n\nfor epoch in range(2):\n    model.train()\n    pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n    for images, masks in pbar:\n        images, masks = images.to(device), masks.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        pbar.set_postfix({\"Loss\": loss.item()})\n    print(f\"Epoch {epoch+1} complete\")\n\n# Save Model\ntorch.save(model.state_dict(), \"/kaggle/working/vit_segmentation.pth\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n# Fonction pour calculer l'IoU\ndef compute_iou(preds, targets, num_classes):\n    iou = []\n    preds = preds.flatten()\n    targets = targets.flatten()\n\n    for cls in range(num_classes):\n        intersection = np.sum((preds == cls) & (targets == cls))\n        union = np.sum((preds == cls) | (targets == cls))\n        iou.append(intersection / (union + 1e-6))  # √âviter la division par z√©ro\n    \n    return np.mean(iou)\n\ndef evaluate(model, dataloader, device, num_classes):\n    model.eval()\n    all_preds = []\n    all_targets = []\n    total_loss = 0\n\n    with torch.no_grad():\n        pbar = tqdm(dataloader, desc=\"Evaluating\")\n        for images, masks in pbar:\n            images = images.to(device)\n            \n            # Si on est en phase de test, masks sera un dictionnaire\n            if masks is not None:\n                masks = masks.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, masks)\n                total_loss += loss.item()\n\n                preds = torch.argmax(outputs, dim=1)\n                all_preds.append(preds.cpu().numpy())\n                all_targets.append(masks.cpu().numpy())\n            else:\n                outputs = model(images)\n                preds = torch.argmax(outputs, dim=1)\n                all_preds.append(preds.cpu().numpy())\n\n        # Calcul des m√©triques\n        all_preds = np.concatenate(all_preds, axis=0)\n        if all_targets:  # Si des cibles sont disponibles (dans le cas d'une phase de test avec des masques)\n            all_targets = np.concatenate(all_targets, axis=0)\n            # Calcul de l'IoU\n            iou = compute_iou(all_preds, all_targets, num_classes)\n            # Calcul des autres m√©triques\n            cm = confusion_matrix(all_targets.flatten(), all_preds.flatten(), labels=np.arange(num_classes))\n            precision = cm.diagonal() / (cm.sum(axis=0) + 1e-6)\n            recall = cm.diagonal() / (cm.sum(axis=1) + 1e-6)\n            f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n            return total_loss / len(dataloader), iou, precision, recall, f1\n        else:\n            # Si pas de masque de test, calculer seulement les pr√©dictions\n            return all_preds\n\n\n# Param√®tres d'√©valuation\nnum_classes = 2  # Ajustez en fonction de votre cas\nmodel = ViTSegmentationModel().to(device)\nmodel.load_state_dict(torch.load(\"/kaggle/working/vit_segmentation.pth\"))\nmodel.eval()\n\n# √âvaluation sur les donn√©es de test\ntest_loss, iou, precision, recall, f1 = evaluate(model, train_dataloader, device, num_classes)\n\n# Affichage des r√©sultats\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Mean IoU: {iou:.4f}\")\nprint(f\"Precision per class: {precision}\")\nprint(f\"Recall per class: {recall}\")\nprint(f\"F1 score per class: {f1}\")\n\n# Optionnel: Afficher une image test avec la pr√©diction\ndef show_sample(model, dataloader, device):\n    model.eval()\n    images, masks = next(iter(dataloader))\n    images, masks = images.to(device), masks.to(device)\n\n    with torch.no_grad():\n        outputs = model(images)\n        preds = torch.argmax(outputs, dim=1)\n\n    image = images[0].cpu().numpy().transpose(1, 2, 0)  # Convertir en image\n    mask = masks[0].cpu().numpy()\n    pred = preds[0].cpu().numpy()\n\n    fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n    ax[0].imshow(image)\n    ax[0].set_title(\"Image\")\n    ax[1].imshow(mask)\n    ax[1].set_title(\"True Mask\")\n    ax[2].imshow(pred)\n    ax[2].set_title(\"Predicted Mask\")\n    plt.show()\n\nshow_sample(model, train_dataloader, device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}