{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91683,"databundleVersionId":11007828,"sourceType":"competition"}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport shutil\nimport os\nimport re\n\ndef generate_label_mapping(root_dir, other_dir, input_subdir, output_csv):\n    \"\"\"\n    Generate a CSV mapping input chips to corresponding segmentation maps.\n\n    Args:\n        root_dir (str or Path): Root directory containing the subdirectories for chips and segmentation maps.\n        input_subdir (str): Subdirectory path for chips within the root directory.\n        output_csv (str or Path): Output path for the generated CSV file.\n    \"\"\"\n    root_dir = Path(root_dir)\n    chips_orig = os.listdir(root_dir / input_subdir / \"chips\")\n\n    chips = [chip.replace(\"chip\", f\"{input_subdir}/chips/chip\") for chip in chips_orig]\n    seg_maps = [chip.replace(\"chip\", f\"{input_subdir}/seg_maps/seg_map\") for chip in chips_orig]\n\n    df = pd.DataFrame({\"Input\": chips, \"Label\": seg_maps})\n    df.to_csv(other_dir + '/' + output_csv, index=False)\n    \n    print(f\"Number of rows is: {df.shape[0]}\")\n    print(f\"CSV generated and saved to: {root_dir / output_csv}\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T13:56:35.105980Z","iopub.execute_input":"2025-02-05T13:56:35.106315Z","iopub.status.idle":"2025-02-05T13:56:35.113114Z","shell.execute_reply.started":"2025-02-05T13:56:35.106281Z","shell.execute_reply":"2025-02-05T13:56:35.111903Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"generate_label_mapping('/kaggle/input/geo-ai-hack/', '/kaggle/working/',\"s2_train/s2_train/\", \"s2_train_ds.csv\")\ngenerate_label_mapping('/kaggle/input/geo-ai-hack/', '/kaggle/working/', \"s2_test/s2_test/\", \"s2_test_ds.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T13:56:52.808811Z","iopub.execute_input":"2025-02-05T13:56:52.809128Z","iopub.status.idle":"2025-02-05T13:56:52.908076Z","shell.execute_reply.started":"2025-02-05T13:56:52.809100Z","shell.execute_reply":"2025-02-05T13:56:52.906796Z"}},"outputs":[{"name":"stdout","text":"Number of rows is: 11764\nCSV generated and saved to: /kaggle/input/geo-ai-hack/s2_train_ds.csv\nNumber of rows is: 3937\nCSV generated and saved to: /kaggle/input/geo-ai-hack/s2_test_ds.csv\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"!pip install rasterio tqdm\n\nimport os\nimport torch\nimport numpy as np\nimport rasterio\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom torchvision.models.segmentation import deeplabv3_resnet50\nimport torch.optim as optim\nimport torch.nn as nn\nimport pandas as pd\nfrom tqdm import tqdm  # Import tqdm for the progress bar\n\n# Custom Dataset Class for 8-Channel Segmentation\nclass CustomSegmentationDataset(Dataset):\n    def __init__(self, image_paths, mask_paths=None, transform=None, is_test=False):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths if not is_test else None\n        self.transform = transform\n        self.is_test = is_test\n        self.samples = []\n\n        if not is_test:\n            for img_path, mask_path in zip(image_paths, mask_paths):\n                if os.path.exists(img_path) and os.path.exists(mask_path):\n                    self.samples.append((img_path, mask_path))\n        else:\n            for img_path in image_paths:\n                if os.path.exists(img_path):\n                    self.samples.append((img_path, None))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        image_path, mask_path = self.samples[idx]\n        preprocessing = Preprocessing(image_path)\n        image = preprocessing.preprocess_image()\n        image = torch.tensor(image.transpose(2, 0, 1), dtype=torch.float32)  # (C, H, W)\n\n        if self.is_test:\n            return image, {\"image_id\": torch.tensor([idx])}\n        else:\n            mask = plt.imread(mask_path)\n\n            # Convert mask to grayscale (if it has extra channels)\n            if mask.ndim == 3:\n                mask = mask[..., 0]  # Take only the first channel\n                \n            # Create a writable copy of the mask\n            mask = mask.copy()  # Make the mask writable\n\n            # Convert mask values to integers (0 for background, 1 for class)\n            mask[mask == -9999] = 0  # Set -9999 as background\n            mask[mask == 1] = 1       # Set 1 as the foreground class\n\n            # Ensure it's a 2D tensor (H, W) and correct dtype\n            mask = torch.tensor(mask, dtype=torch.long).squeeze(0)\n\n            if self.transform:\n                image = self.transform(image)\n                \n            print(mask.shape)  # Check the shape of the mask\n\n            return image, mask  # Image (C, H, W), Mask (H, W)\n\n# Preprocessing Class for 8 Channels\nclass Preprocessing:\n    def __init__(self, image_path):\n        self.image_path = image_path\n\n    def load_bands(self):\n        with rasterio.open(self.image_path) as src:\n            blue = src.read(1)\n            green = src.read(2)\n            red = src.read(3)\n            nir = src.read(4)\n            swir1 = src.read(5)\n            swir2 = src.read(6)\n        return blue, green, red, nir, swir1, swir2\n\n    def preprocess_image(self):\n        blue, green, red, nir, swir1, swir2 = self.load_bands()\n        ndvi = self.compute_ndvi(red, nir)\n        evi = self.compute_evi(nir, red, blue)\n        normalized_bands = [self.normalize_band(band) for band in [blue, green, red, nir, swir1, swir2]]\n        image = np.stack(normalized_bands + [ndvi, evi], axis=-1)  # Stack 8 channels\n        return image\n\n    def normalize_band(self, band):\n        return (band - np.min(band)) / (np.max(band) - np.min(band))\n\n    def compute_ndvi(self, red, nir):\n        return (nir - red) / (nir + red + 1e-6)\n\n    def compute_evi(self, nir, red, blue, g=2.5, c1=6, c2=7.5, l=1):\n        return np.clip(g * (nir - red) / (nir + c1 * red - c2 * blue + l), 0, 1)\n\n# Training and Model Setup\nclass DeepLabV3Model:\n    def __init__(self, num_classes=2, device='cuda'):\n        self.device = device\n        self.model = deeplabv3_resnet50(pretrained=True)\n\n        # Modify input layer to accept 8 channels\n        in_features = self.model.backbone.conv1.in_channels\n        self.model.backbone.conv1 = nn.Conv2d(8, 64, kernel_size=7, stride=2, padding=3, bias=False)\n\n        # Modify output layer for segmentation classes\n        self.model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)\n\n        self.model.to(self.device)\n        self.criterion = nn.CrossEntropyLoss()\n        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n\n    def train(self, dataloader, num_epochs=10, checkpoint_interval=5):\n        self.model.train()\n        for epoch in range(num_epochs):\n            running_loss = 0.0\n            # Initialize tqdm for progress bar\n            pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True)\n\n            for images, masks in pbar:\n                images, masks = images.to(self.device), masks.to(self.device)\n                self.optimizer.zero_grad()\n                outputs = self.model(images)['out']\n                loss = self.criterion(outputs, masks)\n                loss.backward()\n                self.optimizer.step()\n                running_loss += loss.item()\n\n                # Update the progress bar description with the current loss\n                pbar.set_postfix({\"Loss\": running_loss / (pbar.n + 1)})\n\n            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(dataloader):.4f}\")\n            \n            # Save checkpoint every `checkpoint_interval` epochs\n            if (epoch + 1) % checkpoint_interval == 0:\n                self.save_checkpoint(epoch + 1)\n\n    def evaluate(self, dataloader):\n        self.model.eval()\n        iou_scores = []\n        with torch.no_grad():\n            for images, masks in dataloader:\n                images, masks = images.to(self.device), masks.to(self.device)\n                outputs = self.model(images)['out']\n                preds = torch.argmax(outputs, dim=1)\n                intersection = (preds & masks).float().sum()\n                union = (preds | masks).float().sum()\n                iou_scores.append(intersection / union)\n        mean_iou = sum(iou_scores) / len(iou_scores)\n        print(f\"Mean IoU: {mean_iou:.4f}\")\n\n    def save_checkpoint(self, epoch):\n        checkpoint_path = f\"checkpoint_epoch_{epoch}.pth\"\n        torch.save(self.model.state_dict(), checkpoint_path)\n        print(f\"Checkpoint saved at {checkpoint_path}\")\n\n# Load dataset\ntrain_csv = pd.read_csv(\"/kaggle/working/s2_train_ds.csv\")\ntest_csv = pd.read_csv(\"/kaggle/working/s2_test_ds.csv\")\n\ntrain_image_paths = train_csv[\"Input\"].tolist()\ntrain_mask_paths = train_csv[\"Label\"].tolist()\ntest_image_paths = test_csv[\"Input\"].tolist()\n\n# Data Transformations\ntransform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n])\n%cd /kaggle/input/geo-ai-hack\n\n# Create Datasets and Dataloaders\ntrain_dataset = CustomSegmentationDataset(image_paths=train_image_paths, mask_paths=train_mask_paths, transform=transform)\ntest_dataset = CustomSegmentationDataset(image_paths=test_image_paths, is_test=True, transform=transform)\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n\n# Initialize and train the model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DeepLabV3Model(device=device)\nmodel.train(train_dataloader, num_epochs=10, checkpoint_interval=5)\n\n# Evaluate the model\nmodel.evaluate(test_dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T13:57:18.607952Z","iopub.execute_input":"2025-02-05T13:57:18.608271Z"}},"outputs":[{"name":"stdout","text":"Collecting rasterio\n  Downloading rasterio-1.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\nCollecting affine (from rasterio)\n  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\nRequirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (24.3.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2024.12.14)\nRequirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.7)\nRequirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from rasterio) (0.7.2)\nRequirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.26.4)\nRequirement already satisfied: click-plugins in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.1.1)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from rasterio) (3.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->rasterio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->rasterio) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->rasterio) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24->rasterio) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24->rasterio) (2024.2.0)\nDownloading rasterio-1.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading affine-2.4.0-py3-none-any.whl (15 kB)\nInstalling collected packages: affine, rasterio\nSuccessfully installed affine-2.4.0 rasterio-1.4.3\n/kaggle/input/geo-ai-hack\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/deeplabv3_resnet50_coco-cd0a2569.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_resnet50_coco-cd0a2569.pth\n100%|██████████| 161M/161M [00:01<00:00, 161MB/s]  \nEpoch 1/10:   0%|          | 0/1471 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-16-ff5d4134510d>:99: RuntimeWarning: divide by zero encountered in divide\n  return np.clip(g * (nir - red) / (nir + c1 * red - c2 * blue + l), 0, 1)\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   0%|          | 1/1471 [00:23<9:43:20, 23.81s/it, Loss=0.669]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   0%|          | 2/1471 [00:46<9:22:54, 22.99s/it, Loss=0.639]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   0%|          | 3/1471 [01:08<9:09:49, 22.47s/it, Loss=0.601]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   0%|          | 4/1471 [01:29<8:58:19, 22.02s/it, Loss=0.566]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   0%|          | 5/1471 [01:51<8:58:17, 22.03s/it, Loss=0.531]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   0%|          | 6/1471 [02:12<8:52:05, 21.79s/it, Loss=0.503]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   0%|          | 7/1471 [02:35<8:55:35, 21.95s/it, Loss=0.47] ","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   1%|          | 8/1471 [02:56<8:51:49, 21.81s/it, Loss=0.441]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   1%|          | 9/1471 [03:17<8:48:14, 21.68s/it, Loss=0.422]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   1%|          | 10/1471 [03:39<8:45:11, 21.57s/it, Loss=0.397]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-16-ff5d4134510d>:93: RuntimeWarning: invalid value encountered in divide\n  return (band - np.min(band)) / (np.max(band) - np.min(band))\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   1%|          | 11/1471 [04:01<8:47:12, 21.67s/it, Loss=nan]  ","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   1%|          | 12/1471 [04:22<8:45:07, 21.60s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   1%|          | 13/1471 [04:43<8:42:30, 21.50s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   1%|          | 14/1471 [05:05<8:40:50, 21.45s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   1%|          | 15/1471 [05:26<8:39:01, 21.39s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   1%|          | 16/1471 [05:47<8:35:45, 21.27s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   1%|          | 17/1471 [06:08<8:35:41, 21.28s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   1%|          | 18/1471 [06:30<8:36:47, 21.34s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   1%|▏         | 19/1471 [06:51<8:34:27, 21.26s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   1%|▏         | 20/1471 [07:13<8:38:50, 21.45s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   1%|▏         | 21/1471 [07:34<8:40:42, 21.55s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   1%|▏         | 22/1471 [07:56<8:36:45, 21.40s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   2%|▏         | 23/1471 [08:17<8:36:14, 21.39s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   2%|▏         | 24/1471 [08:38<8:35:23, 21.37s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   2%|▏         | 25/1471 [09:02<8:49:18, 21.96s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   2%|▏         | 26/1471 [09:25<8:59:54, 22.42s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   2%|▏         | 27/1471 [09:48<9:05:33, 22.67s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   2%|▏         | 28/1471 [10:11<9:05:17, 22.67s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   2%|▏         | 29/1471 [10:34<9:05:55, 22.72s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   2%|▏         | 30/1471 [10:57<9:09:02, 22.86s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   2%|▏         | 31/1471 [11:18<8:57:57, 22.42s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   2%|▏         | 32/1471 [11:39<8:44:37, 21.87s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   2%|▏         | 33/1471 [12:00<8:39:21, 21.67s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   2%|▏         | 34/1471 [12:21<8:35:55, 21.54s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   2%|▏         | 35/1471 [12:42<8:26:57, 21.18s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   2%|▏         | 36/1471 [13:03<8:26:00, 21.16s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   3%|▎         | 37/1471 [13:24<8:26:09, 21.18s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   3%|▎         | 38/1471 [13:45<8:22:47, 21.05s/it, Loss=nan]","output_type":"stream"},{"name":"stdout","text":"torch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\ntorch.Size([256, 256])\n","output_type":"stream"}],"execution_count":null}]}