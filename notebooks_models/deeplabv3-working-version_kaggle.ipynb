{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:19:19.438299Z",
     "iopub.status.busy": "2025-02-06T10:19:19.437963Z",
     "iopub.status.idle": "2025-02-06T10:20:15.856206Z",
     "shell.execute_reply": "2025-02-06T10:20:15.855273Z",
     "shell.execute_reply.started": "2025-02-06T10:19:19.438270Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rasterio in /usr/local/lib/python3.10/dist-packages (1.4.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
      "Requirement already satisfied: affine in /usr/local/lib/python3.10/dist-packages (from rasterio) (2.4.0)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (24.3.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2024.12.14)\n",
      "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.7)\n",
      "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from rasterio) (0.7.2)\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.26.4)\n",
      "Requirement already satisfied: click-plugins in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.1.1)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from rasterio) (3.2.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->rasterio) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->rasterio) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->rasterio) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->rasterio) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24->rasterio) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24->rasterio) (2024.2.0)\n",
      "/kaggle/input/geo-ai-hack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "<ipython-input-82-8496e1b3cdff>:100: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.clip(g * (nir - red) / (nir + c1 * red - c2 * blue + l), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 0.2015\n"
     ]
    }
   ],
   "source": [
    "!pip install rasterio tqdm\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "# Custom Dataset Class for 8-Channel Segmentation\n",
    "class CustomSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths=None, transform=None, is_test=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths if not is_test else None\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.samples = []\n",
    "\n",
    "        if not is_test:\n",
    "            for img_path, mask_path in zip(image_paths, mask_paths):\n",
    "                if os.path.exists(img_path) and os.path.exists(mask_path):\n",
    "                    self.samples.append((img_path, mask_path))\n",
    "        else:\n",
    "            for img_path in image_paths:\n",
    "                if os.path.exists(img_path):\n",
    "                    self.samples.append((img_path, None))\n",
    "                    \n",
    "        # ðŸ”¥ Ne garder que les x premiers Ã©chantillons\n",
    "        self.samples = self.samples[:1000] # For debugging purposes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, mask_path = self.samples[idx]\n",
    "        preprocessing = Preprocessing(image_path)\n",
    "        image = preprocessing.preprocess_image()\n",
    "        image = torch.tensor(image.transpose(2, 0, 1), dtype=torch.float32)  # (C, H, W)\n",
    "\n",
    "        if self.is_test:\n",
    "            return image, {\"image_id\": torch.tensor([idx])}\n",
    "        else:\n",
    "            mask = plt.imread(mask_path)\n",
    "\n",
    "            # Convert mask to grayscale (if it has extra channels)\n",
    "            if mask.ndim == 3:\n",
    "                mask = mask[..., 0]  # Take only the first channel\n",
    "                \n",
    "            # Create a writable copy of the mask\n",
    "            mask = mask.copy()  # Make the mask writable\n",
    "\n",
    "            # Convert mask values to integers (0 for background, 1 for class)\n",
    "            mask[mask == -9999] = 0  # Set -9999 as background\n",
    "\n",
    "            # Ensure it's a 2D tensor (H, W) and correct dtype\n",
    "            mask = torch.tensor(mask, dtype=torch.long).squeeze(0)\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "            return image, mask  # Image (C, H, W), Mask (H, W)\n",
    "\n",
    "# Preprocessing Class for 8 Channels\n",
    "class Preprocessing:\n",
    "    def __init__(self, image_path):\n",
    "        self.image_path = image_path\n",
    "\n",
    "    def load_bands(self):\n",
    "        with rasterio.open(self.image_path) as src:\n",
    "            blue = src.read(1)\n",
    "            green = src.read(2)\n",
    "            red = src.read(3)\n",
    "            nir = src.read(4)\n",
    "            swir1 = src.read(5)\n",
    "            swir2 = src.read(6)\n",
    "        return blue, green, red, nir, swir1, swir2\n",
    "\n",
    "    def preprocess_image(self):\n",
    "        blue, green, red, nir, swir1, swir2 = self.load_bands()\n",
    "        ndvi = self.compute_ndvi(red, nir)\n",
    "        evi = self.compute_evi(nir, red, blue)\n",
    "        normalized_bands = [self.normalize_band(band) for band in [blue, green, red, nir, swir1, swir2]]\n",
    "        image = np.stack(normalized_bands + [ndvi, evi], axis=-1)  # Stack 8 channels\n",
    "        return image\n",
    "\n",
    "    def normalize_band(self, band):\n",
    "        return (band - np.min(band)) / (np.max(band) - np.min(band))\n",
    "\n",
    "    def compute_ndvi(self, red, nir):\n",
    "        return (nir - red) / (nir + red + 1e-6)\n",
    "\n",
    "    def compute_evi(self, nir, red, blue, g=2.5, c1=6, c2=7.5, l=1):\n",
    "        return np.clip(g * (nir - red) / (nir + c1 * red - c2 * blue + l), 0, 1)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "\n",
    "class DeepLabV3Model(nn.Module):  # Inherit from nn.Module\n",
    "    def __init__(self, num_classes=2, device='cuda'):\n",
    "        super(DeepLabV3Model, self).__init__()  # Initialize nn.Module\n",
    "        self.device = device\n",
    "        self.model = deeplabv3_resnet50(pretrained=True)\n",
    "\n",
    "        # Modify input layer to accept 8 channels\n",
    "        in_features = self.model.backbone.conv1.in_channels\n",
    "        self.model.backbone.conv1 = nn.Conv2d(8, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        # Modify output layer for segmentation classes\n",
    "        self.model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "    def forward(self, x):  # Define the forward pass\n",
    "        return self.model(x)\n",
    "\n",
    "    def train_model(self, dataloader, num_epochs=10, checkpoint_interval=5):\n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            for images, masks in dataloader:\n",
    "                images, masks = images.to(self.device), masks.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(images)['out']  # Use the model for predictions\n",
    "                loss = self.criterion(outputs, masks)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(dataloader):.4f}\")\n",
    "            if (epoch + 1) % checkpoint_interval == 0:\n",
    "                self.save_checkpoint(epoch + 1)\n",
    "\n",
    "    def save_checkpoint(self, epoch):\n",
    "        checkpoint_path = f\"checkpoint_epoch_{epoch}.pth\"\n",
    "        torch.save(self.model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    def save_model(self, path=\"/kaggle/working/deeplabv3_full_model.pth\"):\n",
    "        torch.save(self.model, path)\n",
    "        print(f\"Model saved at {path}\")\n",
    "\n",
    "    def load_model(self, path=\"/kaggle/working/deeplabv3_full_model.pth\"):\n",
    "        self.model = torch.load(path)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        print(f\"Model loaded from {path}\")\n",
    "\n",
    "\n",
    "    def save_checkpoint(self, epoch):\n",
    "        checkpoint_path = f\"checkpoint_epoch_{epoch}.pth\"\n",
    "        torch.save(self.model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    def save_model(self, path=\"/kaggle/working/deeplabv3_full_model.pth\"):\n",
    "        \"\"\"\n",
    "        Save the entire model (architecture + weights)\n",
    "        \"\"\"\n",
    "        torch.save(self.model, path)\n",
    "        print(f\"Model saved at {path}\")\n",
    "        \n",
    "    def load_model(self, path=\"/kaggle/working/deeplabv3_full_model.pth\"):\n",
    "        \"\"\"\n",
    "        Load the entire model (architecture + weights)\n",
    "        \"\"\"\n",
    "        self.model = torch.load(path)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        print(f\"Model loaded from {path}\")\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "train_csv = pd.read_csv(\"/kaggle/input/train-test/train_ds.csv\")\n",
    "test_csv = pd.read_csv(\"/kaggle/input/train-test/test_ds.csv\")\n",
    "\n",
    "train_image_paths = train_csv[\"Input\"].tolist()\n",
    "train_mask_paths = train_csv[\"Label\"].tolist()\n",
    "test_image_paths = test_csv[\"Input\"].tolist()\n",
    "\n",
    "# Data Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "])\n",
    "%cd /kaggle/input/geo-ai-hack\n",
    "\n",
    "# Create Datasets and Dataloaders\n",
    "train_dataset = CustomSegmentationDataset(image_paths=train_image_paths, mask_paths=train_mask_paths, transform=transform)\n",
    "test_dataset = CustomSegmentationDataset(image_paths=test_image_paths, is_test=True, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Initialize and train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeepLabV3Model(device=device) # model = DeepLabV3Model(device=device, checkpoint_dir=\".\") # Checkpoint dir to remove if this code not updated to checkpoint\n",
    "model.train_model(train_dataloader, num_epochs=1, checkpoint_interval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:20:40.645380Z",
     "iopub.status.busy": "2025-02-06T10:20:40.645056Z",
     "iopub.status.idle": "2025-02-06T10:21:02.305712Z",
     "shell.execute_reply": "2025-02-06T10:21:02.304652Z",
     "shell.execute_reply.started": "2025-02-06T10:20:40.645346Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-82-8496e1b3cdff>:100: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.clip(g * (nir - red) / (nir + c1 * red - c2 * blue + l), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to /kaggle/working/hls_submissions.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Specify the output file path\n",
    "OUTPUT_DIR = '/kaggle/working/'\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_DIR, 'hls_submissions.csv')\n",
    "\n",
    "def evaluate_and_save(model, dataloader, output_file):\n",
    "    model.model.eval()\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = torch.stack([image.to(model.device) for image in images])  # Convert list to tensor\n",
    "            predictions = model.model(images)['out']  # Get model predictions\n",
    "\n",
    "            # Iterate over each image in the batch\n",
    "            for idx, prediction in enumerate(predictions):\n",
    "                # Extract image_id correctly from targets\n",
    "                image_id = targets[\"image_id\"].flatten()[idx].item()\n",
    "\n",
    "                # Get the highest score and predicted class\n",
    "                max_scores, predicted_classes = prediction.max(0)  # Max across classes (0th axis)\n",
    "                max_score = max_scores.flatten().max().item()  # Max score for the whole image\n",
    "\n",
    "                # Confidence threshold: If max score > 0.5, consider it positive (class 1)\n",
    "                label = 1 if max_score > 0.5 else 0\n",
    "\n",
    "                # Append the result (image_id, label) to the results list\n",
    "                results.append([image_id, label])\n",
    "\n",
    "    # Save the results to the output CSV file\n",
    "    with open(output_file, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"id\", \"prediction\"])  # Write header\n",
    "        writer.writerows(results)  # Write the results\n",
    "\n",
    "    print(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "# Call the evaluation function\n",
    "evaluate_and_save(model, test_dataloader, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:21:02.306806Z",
     "iopub.status.busy": "2025-02-06T10:21:02.306592Z",
     "iopub.status.idle": "2025-02-06T10:21:27.471823Z",
     "shell.execute_reply": "2025-02-06T10:21:27.470996Z",
     "shell.execute_reply.started": "2025-02-06T10:21:02.306787Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-82-8496e1b3cdff>:100: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.clip(g * (nir - red) / (nir + c1 * red - c2 * blue + l), 0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC: 0.5060\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_train_auc(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(model.device), masks.to(model.device)\n",
    "            outputs = model(images)['out']  # Model output (logits or probabilities)\n",
    "            \n",
    "            # Get probabilities by applying softmax to the logits\n",
    "            softmax = torch.nn.Softmax(dim=1)\n",
    "            prob_outputs = softmax(outputs)\n",
    "\n",
    "            # Flatten the masks and predictions\n",
    "            all_labels.append(masks.cpu().numpy().flatten())\n",
    "            all_preds.append(prob_outputs.cpu().numpy().reshape(-1, prob_outputs.shape[1]))\n",
    "    # Concatenate all labels and predictions from the batch\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_preds = np.argmax(np.concatenate(all_preds), axis = 1)\n",
    "    \n",
    "    # Check if all_labels is a binary or multi-class problem\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Train AUC: {auc:.4f}\")\n",
    "    return auc\n",
    "\n",
    "\n",
    "# Call this function with the train dataloader\n",
    "train_auc = evaluate_train_auc(model, train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:21:33.855632Z",
     "iopub.status.busy": "2025-02-06T10:21:33.855317Z",
     "iopub.status.idle": "2025-02-06T10:21:34.104369Z",
     "shell.execute_reply": "2025-02-06T10:21:34.103559Z",
     "shell.execute_reply.started": "2025-02-06T10:21:33.855607Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at /kaggle/working/deeplabv3_full_model.pth\n"
     ]
    }
   ],
   "source": [
    "model.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11007828,
     "sourceId": 91683,
     "sourceType": "competition"
    },
    {
     "datasetId": 6608035,
     "sourceId": 10669263,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
