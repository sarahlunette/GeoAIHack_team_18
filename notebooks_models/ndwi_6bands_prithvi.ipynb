{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91683,"databundleVersionId":11007828,"sourceType":"competition"}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Clone the InstaGeo-E2E-Geospatial-ML repository from GitHub\nrepository_url = \"https://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML\"\n!git clone {repository_url}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:32:21.586579Z","iopub.execute_input":"2025-02-05T12:32:21.586787Z","iopub.status.idle":"2025-02-05T12:32:22.398971Z","shell.execute_reply.started":"2025-02-05T12:32:21.586767Z","shell.execute_reply":"2025-02-05T12:32:22.397899Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'InstaGeo-E2E-Geospatial-ML'...\nremote: Enumerating objects: 374, done.\u001b[K\nremote: Counting objects: 100% (105/105), done.\u001b[K\nremote: Compressing objects: 100% (44/44), done.\u001b[K\nremote: Total 374 (delta 80), reused 61 (delta 61), pack-reused 269 (from 1)\u001b[K\nReceiving objects: 100% (374/374), 1.42 MiB | 10.33 MiB/s, done.\nResolving deltas: 100% (207/207), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%bash\ncd InstaGeo-E2E-Geospatial-ML\ngit pull","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:32:22.400125Z","iopub.execute_input":"2025-02-05T12:32:22.400473Z","iopub.status.idle":"2025-02-05T12:32:22.622160Z","shell.execute_reply.started":"2025-02-05T12:32:22.400424Z","shell.execute_reply":"2025-02-05T12:32:22.621333Z"}},"outputs":[{"name":"stdout","text":"Already up to date.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%capture\n%%bash\n# Navigate to the cloned InstaGeo-E2E-Geospatial-ML directory\ncd /kaggle/working/InstaGeo-E2E-Geospatial-ML\n# Stash any local changes to avoid conflicts when switching branches\ngit stash\n#Switch to the 'geo-ai-hack' branch, which likely contains specific code for the Geo AI Hackathon\ngit checkout geo-ai-hack\n# Install the InstaGeo package \npip install -e .[all]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:32:22.623095Z","iopub.execute_input":"2025-02-05T12:32:22.623424Z","iopub.status.idle":"2025-02-05T12:32:59.318926Z","shell.execute_reply.started":"2025-02-05T12:32:22.623393Z","shell.execute_reply":"2025-02-05T12:32:59.318105Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport re\nimport shutil\nimport yaml\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom pyproj import CRS, Transformer\nimport rasterio\nos.environ[\"HYDRA_FULL_ERROR\"] =\"1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:32:59.320609Z","iopub.execute_input":"2025-02-05T12:32:59.320858Z","iopub.status.idle":"2025-02-05T12:33:00.423556Z","shell.execute_reply.started":"2025-02-05T12:32:59.320823Z","shell.execute_reply":"2025-02-05T12:33:00.422671Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import os\nimport re\nimport shutil\nimport os\nimport shutil\nfrom pathlib import Path\n\ndef copy_last_25_percent_with_seg_maps(\n    input_chips_dir,\n    input_segmaps_dir,\n    output_chips_dir,\n    output_segmaps_dir\n):\n    \"\"\"\n    Copie les 25% de fichiers .tif \"depuis la fin\" du dossier `input_chips_dir`\n    vers `output_chips_dir`, et copie aussi les seg_maps correspondantes\n    (même nom, en remplaçant \"chip_\" par \"seg_map_\").\n\n    Exemple:\n      - input_chips_dir = \"/kaggle/input/hls_train/hls_train/chips\"\n      - input_segmaps_dir = \"/kaggle/input/hls_train/hls_train/seg_maps\"\n      - output_chips_dir = \"/kaggle/working/hls_train_subset/hls_train_subset/chips\"\n      - output_segmaps_dir = \"/kaggle/working/hls_train_subset/hls_train_subset/seg_maps\"\n    \"\"\"\n    input_chips_dir = Path(input_chips_dir)\n    input_segmaps_dir = Path(input_segmaps_dir)\n    output_chips_dir = Path(output_chips_dir)\n    output_segmaps_dir = Path(output_segmaps_dir)\n\n    output_chips_dir.mkdir(parents=True, exist_ok=True)\n    output_segmaps_dir.mkdir(parents=True, exist_ok=True)\n\n    # Lister tous les TIF dans chips\n    all_chip_files = [f for f in os.listdir(input_chips_dir) if f.endswith(\".tif\")]\n    all_chip_files.sort()  # Tri alphabétique (si votre naming est chronologique, ça marche)\n    total = len(all_chip_files)\n    if total == 0:\n        print(f\"[WARNING] Aucun TIFF dans {input_chips_dir}\")\n        return\n\n    # Calcul du nombre de fichiers à prendre (25%)\n    subset_count = int(total * 0.31)\n    # On prend les \"derniers\" subset_count\n    subset_chips = all_chip_files[-subset_count:]  # depuis la fin\n\n    print(f\"[INFO] Trouvé {total} TIFF dans {input_chips_dir}\")\n    print(f\"       On copie les {subset_count} derniers vers {output_chips_dir}\")\n\n    copied = 0\n    for chip_file in subset_chips:\n        src_chip = input_chips_dir / chip_file\n        dst_chip = output_chips_dir / chip_file\n        shutil.copy2(src_chip, dst_chip)\n        copied += 1\n\n        # Copier la seg_map correspondante si elle existe\n        seg_file = chip_file.replace(\"chip_\", \"seg_map_\")\n        seg_src = input_segmaps_dir / seg_file\n        if seg_src.exists():\n            dst_seg = output_segmaps_dir / seg_file\n            shutil.copy2(seg_src, dst_seg)\n\n    print(f\"[OK] {copied} fichiers copiés dans {output_chips_dir} + seg_maps associées.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:33:00.427239Z","iopub.execute_input":"2025-02-05T12:33:00.427473Z","iopub.status.idle":"2025-02-05T12:33:00.434534Z","shell.execute_reply.started":"2025-02-05T12:33:00.427453Z","shell.execute_reply":"2025-02-05T12:33:00.433692Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"copy_last_25_percent_with_seg_maps(\n    input_chips_dir   = \"/kaggle/input/geo-ai-hack/hls_train/hls_train/chips\",\n    input_segmaps_dir = \"/kaggle/input/geo-ai-hack/hls_train/hls_train/seg_maps\",\n    output_chips_dir  = \"/kaggle/working/hls_train_subset/hls_train_subset/chips\",\n    output_segmaps_dir= \"/kaggle/working/hls_train_subset/hls_train_subset/seg_maps\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:33:00.435624Z","iopub.execute_input":"2025-02-05T12:33:00.435983Z","iopub.status.idle":"2025-02-05T12:35:01.462259Z","shell.execute_reply.started":"2025-02-05T12:33:00.435950Z","shell.execute_reply":"2025-02-05T12:35:01.461465Z"}},"outputs":[{"name":"stdout","text":"[INFO] Trouvé 10428 TIFF dans /kaggle/input/geo-ai-hack/hls_train/hls_train/chips\n       On copie les 3232 derniers vers /kaggle/working/hls_train_subset/hls_train_subset/chips\n[OK] 3232 fichiers copiés dans /kaggle/working/hls_train_subset/hls_train_subset/chips + seg_maps associées.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport numpy as np\nimport rasterio\nfrom pathlib import Path\n\ndef replace_swir2_with_ndwi_inplace_multitime_no_spike(chips_dir):\n    \"\"\"\n    Pour chaque TIFF dans chips_dir (shape [18, H, W] = 3 time steps × 6 bandes):\n      - Lecture en mémoire, suppression immédiate du fichier\n      - Calcul NDWI pour chaque time step:\n         NDWI = (NIR - SWIR1) / (NIR + SWIR1 + 1e-6)\n      - On remplace la bande SWIR2 (index 5) par NDWI\n      - On réécrit au même chemin un TIFF de shape [18, H, W],\n        où la bande 5 (SWIR2) est devenue NDWI.\n\n    Structure initiale par time step (6 bandes):\n       0 = Blue\n       1 = Green\n       2 = Red\n       3 = NIR\n       4 = SWIR1\n       5 = SWIR2  --> on va la remplacer par NDWI\n    \"\"\"\n\n    chips_dir = Path(chips_dir)\n    for chip_file in os.listdir(chips_dir):\n        if not chip_file.endswith(\".tif\"):\n            continue\n\n        chip_path = chips_dir / chip_file\n\n        # 1) Lecture en RAM\n        with rasterio.open(chip_path) as src:\n            array = src.read().astype(np.float32)  # shape (18, H, W)\n            profile = src.profile\n\n        # 2) Supprime aussitôt le TIFF original (libère l'espace)\n        os.remove(chip_path)\n\n        # 3) Vérifions qu'on a 18 canaux (3 time steps × 6 bandes)\n        if array.shape[0] != 18:\n            print(f\"[SKIP] {chip_file}: {array.shape[0]} canaux (pas 18).\")\n            # On réécrit le fichier d'origine pour ne pas le perdre\n            with rasterio.open(chip_path, \"w\", **profile) as dst:\n                dst.write(array)\n            continue\n\n        time_steps = 3\n        bands_per_ts = 6\n\n        # 4) Calcul NDWI pour chacun des 3 pas de temps\n        for t in range(time_steps):\n            offset_in = t * bands_per_ts\n\n            # Indices: \n            # 3 = NIR, 4 = SWIR1, 5 = SWIR2 (à remplacer)\n            nir   = array[offset_in + 3]\n            swir1 = array[offset_in + 4]\n\n            ndwi = (nir - swir1) / (nir + swir1 + 1e-6)\n\n            # Remplace la bande SWIR2 (index 5) par NDWI\n            array[offset_in + 5] = ndwi\n\n        # 5) On garde la même shape (18 canaux)\n        profile.update(dtype=\"float32\", count=18)\n\n        # 6) Écrit le nouveau TIFF\n        with rasterio.open(chip_path, \"w\", **profile) as dst:\n            dst.write(array)\n\n    print(f\"[OK] SWIR2 remplacé par NDWI (in-place, 18 canaux) dans : {chips_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:35:01.463238Z","iopub.execute_input":"2025-02-05T12:35:01.463507Z","iopub.status.idle":"2025-02-05T12:35:01.471354Z","shell.execute_reply.started":"2025-02-05T12:35:01.463476Z","shell.execute_reply":"2025-02-05T12:35:01.470538Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"replace_swir2_with_ndwi_inplace_multitime_no_spike(\"/kaggle/working/hls_train_subset/hls_train_subset/chips\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:37:59.586308Z","iopub.execute_input":"2025-02-05T12:37:59.586673Z","iopub.status.idle":"2025-02-05T12:39:28.911011Z","shell.execute_reply.started":"2025-02-05T12:37:59.586643Z","shell.execute_reply":"2025-02-05T12:39:28.910072Z"}},"outputs":[{"name":"stdout","text":"[OK] SWIR2 remplacé par NDWI (in-place, 18 canaux) dans : /kaggle/working/hls_train_subset/hls_train_subset/chips\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"chips_dir = \"/kaggle/working/hls_train_subset/hls_train_subset/chips\"  # adaptez le chemin à votre cas\n\n# Lister tous les fichiers TIFF du dossier\ntif_files = [f for f in os.listdir(chips_dir) if f.endswith(\".tif\")]\n\nif tif_files:\n    first_tif = tif_files[0]\n    first_tif_path = os.path.join(chips_dir, first_tif)\n\n    with rasterio.open(first_tif_path) as src:\n        # Méthode 1: lire la totalité des canaux en mémoire\n        array = src.read()  \n        # array.shape renvoie (nombre_canaux, hauteur, largeur)\n        print(\"array.shape :\", array.shape)\n\n        # Méthode 2: accéder directement aux attributs\n        print(\"src.count  :\", src.count)   # nombre de canaux\n        print(\"src.height :\", src.height)\n        print(\"src.width  :\", src.width)\n\nelse:\n    print(\"Aucun fichier .tif trouvé dans\", chips_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:39:28.912300Z","iopub.execute_input":"2025-02-05T12:39:28.912609Z","iopub.status.idle":"2025-02-05T12:39:28.945558Z","shell.execute_reply.started":"2025-02-05T12:39:28.912578Z","shell.execute_reply":"2025-02-05T12:39:28.944937Z"}},"outputs":[{"name":"stdout","text":"array.shape : (18, 256, 256)\nsrc.count  : 18\nsrc.height : 256\nsrc.width  : 256\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def generate_label_mapping(root_dir, input_subdir, output_csv):\n    \"\"\"\n    Generate a CSV mapping input chips to corresponding segmentation maps.\n\n    Args:\n        root_dir (str or Path): Root directory containing the subdirectories for chips and segmentation maps.\n        input_subdir (str): Subdirectory path for chips within the root directory.\n        output_csv (str or Path): Output path for the generated CSV file.\n    \"\"\"\n    root_dir = Path(root_dir)\n    chips_orig = os.listdir(root_dir / input_subdir / \"chips\")\n    if os.path.exists(root_dir / input_subdir / \"seg_maps\"):\n        add_label = True\n    else:\n        add_label = False\n\n    chips = [chip.replace(\"chip\", f\"{input_subdir}/chips/chip\") for chip in chips_orig]\n\n    if add_label:\n        seg_maps = [chip.replace(\"chip\", f\"{input_subdir}/seg_maps/seg_map\") for chip in chips_orig]\n        df = pd.DataFrame({\"Input\": chips, \"Label\": seg_maps})\n    else:\n        df = pd.DataFrame({\"Input\": chips})\n    df.to_csv(output_csv, index=False)\n    \n    print(f\"Number of rows is: {df.shape[0]}\")\n    print(f\"CSV generated and saved to: {output_csv}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:39:28.947299Z","iopub.execute_input":"2025-02-05T12:39:28.947526Z","iopub.status.idle":"2025-02-05T12:39:28.952935Z","shell.execute_reply.started":"2025-02-05T12:39:28.947507Z","shell.execute_reply":"2025-02-05T12:39:28.952122Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Par exemple, on a transformé /kaggle/working/hls_train/hls_train/chips en 8 bandes\n\nroot_dir = \"/kaggle/working\"  # Le dossier principal\ngenerate_label_mapping(\n    root_dir=root_dir,\n    input_subdir=\"hls_train_subset/hls_train_subset\",  # Chemin vers vos chips \n    output_csv=\"train_ds_subset.csv\"\n)\n\n#il faudra ajouter le test ensuite","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:39:28.954147Z","iopub.execute_input":"2025-02-05T12:39:28.954381Z","iopub.status.idle":"2025-02-05T12:39:28.995580Z","shell.execute_reply.started":"2025-02-05T12:39:28.954357Z","shell.execute_reply":"2025-02-05T12:39:28.994940Z"}},"outputs":[{"name":"stdout","text":"Number of rows is: 3232\nCSV generated and saved to: train_ds_subset.csv\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def split_validation_data(mapping_csv, validation_split=0.3):\n    \"\"\"\n    Split data into training and validation sets based on a CSV file mapping `chips` and `seg_maps`.\n\n    Args:\n        mapping_csv (str or Path): Path to the CSV file containing the mapping between `chips` and `seg_maps`.\n        data_dir (str or Path): Path to the merged directory containing all files.\n        validation_dir (str or Path): Path to the new directory for validation files.\n        validation_split (float): Fraction of the data to use as the validation set.\n    \"\"\"\n    df = pd.read_csv(mapping_csv)\n    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n    num_val = int(len(df) * validation_split)\n    train_df = df[num_val:]\n    val_df = df[:num_val]\n    train_df.to_csv(\"train_split.csv\",index=False)    \n    print(f\"CSV train split  saved to: train_split.csv\")\n    val_df.to_csv(\"validation_split.csv\",index=False)    \n    print(f\"CSV validation split  saved to: validation_split.csv\")\n    \n    return \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:39:28.996318Z","iopub.execute_input":"2025-02-05T12:39:28.996540Z","iopub.status.idle":"2025-02-05T12:39:29.001006Z","shell.execute_reply.started":"2025-02-05T12:39:28.996521Z","shell.execute_reply":"2025-02-05T12:39:29.000204Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"split_validation_data(\"train_ds_subset.csv\", validation_split=0.3)\n# Cela va créer train_split.csv et validation_split.csv\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:39:29.001854Z","iopub.execute_input":"2025-02-05T12:39:29.002117Z","iopub.status.idle":"2025-02-05T12:39:29.052271Z","shell.execute_reply.started":"2025-02-05T12:39:29.002086Z","shell.execute_reply":"2025-02-05T12:39:29.051651Z"}},"outputs":[{"name":"stdout","text":"CSV train split  saved to: train_split.csv\nCSV validation split  saved to: validation_split.csv\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"#on vérifie que notre image a le bon nombre de bandes \n\nwith rasterio.open(\"hls_train_subset/hls_train_subset/chips/chip_20201201_S30_T37QGB_2020304T074041_3_4.tif\") as src : \n    print(src.count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:39:29.053053Z","iopub.execute_input":"2025-02-05T12:39:29.053344Z","iopub.status.idle":"2025-02-05T12:39:29.061819Z","shell.execute_reply.started":"2025-02-05T12:39:29.053316Z","shell.execute_reply":"2025-02-05T12:39:29.061095Z"}},"outputs":[{"name":"stdout","text":"18\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"!python -m instageo.model.run \\\n    --config-name=locust \\\n    root_dir=\"/kaggle/working\" \\\n    train_filepath=\"train_ds_subset.csv\" \\\n    dataloader.temporal_dim=3 \\\n    train.batch_size=8 \\\n    train.num_epochs=1 \\\n    mode=stats \\\n    dataloader.bands=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]\\\n    dataloader.mean=[623.2724609375,1247.657958984375,1772.24169921875,2371.256103515625,2862.867431640625,2357.759765625]\\\n    dataloader.std=[2182.050048828125,2248.420654296875,2302.53515625,2372.204345703125,2398.52685546875,2292.96435546875]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:39:29.063583Z","iopub.execute_input":"2025-02-05T12:39:29.063831Z","iopub.status.idle":"2025-02-05T12:41:47.435165Z","shell.execute_reply.started":"2025-02-05T12:39:29.063803Z","shell.execute_reply":"2025-02-05T12:41:47.434297Z"}},"outputs":[{"name":"stdout","text":"[2025-02-05 12:39:44,080][__main__][INFO] - Script: /kaggle/working/InstaGeo-E2E-Geospatial-ML/instageo/model/run.py\n[2025-02-05 12:39:44,083][__main__][INFO] - Imported hydra config:\nroot_dir: /kaggle/working\nvalid_filepath: null\ntrain_filepath: train_ds_subset.csv\ntest_filepath: null\ncheckpoint_path: null\noutput_dir: null\nmode: stats\ntrain:\n  learning_rate: 0.0001\n  num_epochs: 1\n  batch_size: 8\n  class_weights:\n  - 1\n  - 1\n  ignore_index: -1\n  weight_decay: 0.1\nmodel:\n  freeze_backbone: false\n  num_classes: 2\ndataloader:\n  bands:\n  - 0\n  - 1\n  - 2\n  - 3\n  - 4\n  - 5\n  - 6\n  - 7\n  - 8\n  - 9\n  - 10\n  - 11\n  - 12\n  - 13\n  - 14\n  - 15\n  - 16\n  - 17\n  mean:\n  - 623.2724609375\n  - 1247.657958984375\n  - 1772.24169921875\n  - 2371.256103515625\n  - 2862.867431640625\n  - 2357.759765625\n  std:\n  - 2182.050048828125\n  - 2248.420654296875\n  - 2302.53515625\n  - 2372.204345703125\n  - 2398.52685546875\n  - 2292.96435546875\n  img_size: 256\n  temporal_dim: 3\n  replace_label:\n  - -9999\n  - -1\n  reduce_to_zero: false\n  no_data_value: -9999\n  constant_multiplier: 1.0\ntest:\n  img_size: 256\n  crop_size: 256\n  stride: 256\n  mask_cloud: false\n\n[824.8154907226562, 1380.176513671875, 1866.7628173828125, 2610.20849609375, 3014.04541015625, -60.25557327270508]\n[2098.12451171875, 2137.667724609375, 2160.350830078125, 2211.607666015625, 2191.01220703125, 139875.90625]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def load_yml(filepath):\n    \"\"\"Load data from a YAML file.\n\n    Args:\n        filepath (str | Path): The path to the YAML file.\n\n    Returns:\n        Dict: The loaded data, or None if the file does not exist.\n    \"\"\"\n    filepath=Path(filepath)\n    with filepath.open() as f:\n        return yaml.safe_load(f)\n        \ndef save_yml(data,filepath):\n    \"\"\"Save data to a YAML file.\n\n    Args:\n        data (Dict): The data to save.\n        filepath (str | Path): The file path to save the YAML to.\n    \"\"\"\n    filepath = Path(filepath)\n    with filepath.open(\"w\") as f:\n        yaml.dump(data, f)\n    print(f\"Data saved to {filepath}.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:43:17.579257Z","iopub.execute_input":"2025-02-05T12:43:17.579619Z","iopub.status.idle":"2025-02-05T12:43:17.584667Z","shell.execute_reply.started":"2025-02-05T12:43:17.579595Z","shell.execute_reply":"2025-02-05T12:43:17.583877Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"\nlocust_cfg_path = \"InstaGeo-E2E-Geospatial-ML/instageo/model/configs/locust.yaml\"\nlocust_cfg = load_yml(locust_cfg_path)\n\n# Recopiez les valeurs mean/std découvertes dans la sortie ci-dessus\nlocust_cfg[\"mean\"] = [867.623046875, 1439.7684326171875, 1951.5145263671875, 2704.58984375, 3148.905029296875, -60.25557327270508]  \nlocust_cfg[\"std\"]  = [1995.3145751953125, 2033.8626708984375, 2060.29931640625, 2110.346923828125, 2111.359619140625, 139875.90625]\n\n# Si besoin, configurer in_channels=8 (selon la version d'InstaGeo)\n#locust_cfg[\"model\"][\"in_channels\"] = 8\n\nsave_yml(locust_cfg, locust_cfg_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:43:17.997483Z","iopub.execute_input":"2025-02-05T12:43:17.997728Z","iopub.status.idle":"2025-02-05T12:43:18.012492Z","shell.execute_reply.started":"2025-02-05T12:43:17.997708Z","shell.execute_reply":"2025-02-05T12:43:18.011748Z"}},"outputs":[{"name":"stdout","text":"Data saved to InstaGeo-E2E-Geospatial-ML/instageo/model/configs/locust.yaml.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import pandas as pd\nimport rasterio\nimport os\n\ndf = pd.read_csv(\"train_ds_subset.csv\")\n\nroot_dir = \"/kaggle/working\"  # adaptez si nécessaire\nshapes_count = {}\n\nfor path_rel in df[\"Input\"]:\n    path_abs = os.path.join(root_dir, path_rel)\n    if not os.path.exists(path_abs):\n        print(f\"[WARNING] Fichier introuvable : {path_abs}\")\n        continue\n    with rasterio.open(path_abs) as src:\n        band_count = src.count\n    shapes_count[band_count] = shapes_count.get(band_count, 0) + 1\n\nprint(\"Résumé des shapes rencontrées (nombre de canaux) :\")\nfor band_count, nb_files in shapes_count.items():\n    print(f\" - {band_count} canaux : {nb_files} fichier(s)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:43:21.108324Z","iopub.execute_input":"2025-02-05T12:43:21.108650Z","iopub.status.idle":"2025-02-05T12:43:28.902035Z","shell.execute_reply.started":"2025-02-05T12:43:21.108620Z","shell.execute_reply":"2025-02-05T12:43:28.901202Z"}},"outputs":[{"name":"stdout","text":"Résumé des shapes rencontrées (nombre de canaux) :\n - 18 canaux : 3232 fichier(s)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Train the InstaGeo model using the Locust configuration\n!python -m instageo.model.run  --config-name=locust \\\n    hydra.run.dir=\"/kaggle/working/outputs/first_run\" \\\n    root_dir=\"/kaggle/working\" \\\n    train.batch_size=8 \\\n    train.num_epochs=10\\\n    mode=train \\\n    dataloader.mean=\"[867.623046875, 1439.7684326171875, 1951.5145263671875, 2704.58984375, 3148.905029296875, -60.25557327270508]\"\\\n    dataloader.std=\"[1995.3145751953125, 2033.8626708984375, 2060.29931640625, 2110.346923828125, 2111.359619140625, 139875.90625]\"\\\n    train_filepath=\"train_ds_subset.csv\" \\\n    valid_filepath=\"validation_split.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:43:52.605481Z","iopub.execute_input":"2025-02-05T12:43:52.605792Z"}},"outputs":[{"name":"stdout","text":"[2025-02-05 12:43:58,970][__main__][INFO] - Script: /kaggle/working/InstaGeo-E2E-Geospatial-ML/instageo/model/run.py\n[2025-02-05 12:43:58,974][__main__][INFO] - Imported hydra config:\ncheckpoint_path: null\ndataloader:\n  bands:\n  - 0\n  - 1\n  - 2\n  - 3\n  - 4\n  - 5\n  - 6\n  - 7\n  - 8\n  - 9\n  - 10\n  - 11\n  - 12\n  - 13\n  - 14\n  - 15\n  - 16\n  - 17\n  constant_multiplier: 1.0\n  img_size: 256\n  mean:\n  - 867.623046875\n  - 1439.7684326171875\n  - 1951.5145263671875\n  - 2704.58984375\n  - 3148.905029296875\n  - -60.25557327270508\n  no_data_value: -9999\n  reduce_to_zero: false\n  replace_label:\n  - -9999\n  - -1\n  std:\n  - 1995.3145751953125\n  - 2033.8626708984375\n  - 2060.29931640625\n  - 2110.346923828125\n  - 2111.359619140625\n  - 139875.90625\n  temporal_dim: 3\nmean:\n- 867.623046875\n- 1439.7684326171875\n- 1951.5145263671875\n- 2704.58984375\n- 3148.905029296875\n- -60.25557327270508\nmode: train\nmodel:\n  freeze_backbone: false\n  num_classes: 2\noutput_dir: null\nroot_dir: /kaggle/working\nstd:\n- 1995.3145751953125\n- 2033.8626708984375\n- 2060.29931640625\n- 2110.346923828125\n- 2111.359619140625\n- 139875.90625\ntest:\n  crop_size: 256\n  img_size: 256\n  mask_cloud: false\n  stride: 256\ntest_filepath: null\ntrain:\n  batch_size: 8\n  class_weights:\n  - 1\n  - 1\n  ignore_index: -1\n  learning_rate: 0.0001\n  num_epochs: 10\n  weight_decay: 0.1\ntrain_filepath: train_ds_subset.csv\nvalid_filepath: validation_split.csv\n\n[2025-02-05 12:44:12,608][absl][INFO] - Download successful on attempt 1\n[2025-02-05 12:44:12,792][absl][INFO] - Download successful on attempt 1\n/kaggle/working/InstaGeo-E2E-Geospatial-ML/instageo/model/model.py:155: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(weights_path, map_location=\"cpu\")\n[2025-02-05 12:44:14,871][root][INFO] - GPU is available. Using GPU...\n[2025-02-05 12:44:14,909][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True\n[2025-02-05 12:44:14,910][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores\n[2025-02-05 12:44:14,910][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs\n[2025-02-05 12:44:14,968][lightning_fabric.utilities.distributed][INFO] - Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n[2025-02-05 12:44:21,082][__main__][INFO] - Script: /kaggle/working/InstaGeo-E2E-Geospatial-ML/instageo/model/run.py\n[2025-02-05 12:44:21,086][__main__][INFO] - Imported hydra config:\ncheckpoint_path: null\ndataloader:\n  bands:\n  - 0\n  - 1\n  - 2\n  - 3\n  - 4\n  - 5\n  - 6\n  - 7\n  - 8\n  - 9\n  - 10\n  - 11\n  - 12\n  - 13\n  - 14\n  - 15\n  - 16\n  - 17\n  constant_multiplier: 1.0\n  img_size: 256\n  mean:\n  - 867.623046875\n  - 1439.7684326171875\n  - 1951.5145263671875\n  - 2704.58984375\n  - 3148.905029296875\n  - -60.25557327270508\n  no_data_value: -9999\n  reduce_to_zero: false\n  replace_label:\n  - -9999\n  - -1\n  std:\n  - 1995.3145751953125\n  - 2033.8626708984375\n  - 2060.29931640625\n  - 2110.346923828125\n  - 2111.359619140625\n  - 139875.90625\n  temporal_dim: 3\nmean:\n- 867.623046875\n- 1439.7684326171875\n- 1951.5145263671875\n- 2704.58984375\n- 3148.905029296875\n- -60.25557327270508\nmode: train\nmodel:\n  freeze_backbone: false\n  num_classes: 2\noutput_dir: null\nroot_dir: /kaggle/working\nstd:\n- 1995.3145751953125\n- 2033.8626708984375\n- 2060.29931640625\n- 2110.346923828125\n- 2111.359619140625\n- 139875.90625\ntest:\n  crop_size: 256\n  img_size: 256\n  mask_cloud: false\n  stride: 256\ntest_filepath: null\ntrain:\n  batch_size: 8\n  class_weights:\n  - 1\n  - 1\n  ignore_index: -1\n  learning_rate: 0.0001\n  num_epochs: 10\n  weight_decay: 0.1\ntrain_filepath: train_ds_subset.csv\nvalid_filepath: validation_split.csv\n\n[2025-02-05 12:44:31,922][absl][INFO] - File '/root/.instageo/prithvi/Prithvi_EO_V1_100M.pt' already exists. Skipping download.\n[2025-02-05 12:44:31,922][absl][INFO] - File '/root/.instageo/prithvi/config.yaml' already exists. Skipping download.\n/kaggle/working/InstaGeo-E2E-Geospatial-ML/instageo/model/model.py:155: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(weights_path, map_location=\"cpu\")\n[2025-02-05 12:44:34,372][root][INFO] - GPU is available. Using GPU...\n[2025-02-05 12:44:34,540][lightning_fabric.utilities.distributed][INFO] - Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n[2025-02-05 12:44:34,545][pytorch_lightning.utilities.rank_zero][INFO] - ----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 2 processes\n----------------------------------------------------------------------------------------------------\n\n2025-02-05 12:44:36.714874: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-02-05 12:44:36.893116: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-02-05 12:44:36.943607: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working/outputs/first_run exists and is not empty.\n[2025-02-05 12:44:46,786][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n[2025-02-05 12:44:46,786][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n[2025-02-05 12:44:47,027][pytorch_lightning.callbacks.model_summary][INFO] - \n  | Name      | Type             | Params | Mode \n-------------------------------------------------------\n0 | net       | PrithviSeg       | 134 M  | train\n1 | criterion | CrossEntropyLoss | 0      | train\n-------------------------------------------------------\n133 M     Trainable params\n590 K     Non-trainable params\n134 M     Total params\n537.703   Total estimated model params size (MB)\n198       Modules in train mode\n0         Modules in eval mode\nSanity Checking DataLoader 0: 100%|███████████████| 2/2 [00:02<00:00,  0.87it/s]/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_aAcc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_mIoU', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_IoU_0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_IoU_1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_Acc_0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_Acc_1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_Precision_0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_Precision_1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_Recall_0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_Recall_1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\nEpoch 0:  33%|▎| 67/202 [02:35<05:12,  0.43it/s, v_num=0, train_loss_step=0.673,","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"generate_label_mapping('/kaggle/input/geo-ai-hack', 'hls_test/hls_test', \"test_ds.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:35:02.174683Z","iopub.status.idle":"2025-02-05T12:35:02.175120Z","shell.execute_reply":"2025-02-05T12:35:02.174940Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\npython -m instageo.model.run --config-name=locust \\\n    root_dir=\"/kaggle/input/geo-ai-hack\" \\\n    test_filepath=\"validation_split.csv\" \\\n    train.batch_size=16 \\\n    checkpoint_path='/kaggle/working/outputs/first_run/instageo_best_checkpoint.ckpt' \\\n    mode=eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:35:02.175889Z","iopub.status.idle":"2025-02-05T12:35:02.176258Z","shell.execute_reply":"2025-02-05T12:35:02.176100Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\npython -m instageo.model.run --config-name=locust \\\n    root_dir=\"/kaggle/input/geo-ai-hack\" \\\n    test_filepath=\"test_ds.csv\" \\\n    train.batch_size=16 \\\n    checkpoint_path='/kaggle/working/outputs/first_run/instageo_best_checkpoint.ckpt' \\\n    output_dir='/kaggle/working/predictions' \\\n    mode=chip_inference","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:35:02.176807Z","iopub.status.idle":"2025-02-05T12:35:02.177208Z","shell.execute_reply":"2025-02-05T12:35:02.177040Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\npredictions_directory = \"/kaggle/working/predictions\"\nprediction_files = os.listdir(predictions_directory)\n\ndef get_prediction_value(row):\n    matching_files = [f for f in prediction_files if (str(row['date']) in f) and (row['mgrs_tile_id'] in f)]\n    if not matching_files:\n        return (np.nan, np.nan)\n    for file in matching_files:\n        with rasterio.open(f\"{predictions_directory}/{file}\") as src:\n            width, height = src.width, src.height\n            affine_transform = rasterio.transform.AffineTransformer(src.transform)\n            transformer = Transformer.from_crs(CRS.from_epsg(4326), src.crs, always_xy=True)\n            x_chip, y_chip = transformer.transform(row['x'], row['y'])\n            x_offset, y_offset = affine_transform.rowcol(x_chip, y_chip)\n            \n            if 0 <= x_offset < width and 0 <= y_offset < height:\n                return src.read(1)[y_offset, x_offset], file\n    return (np.nan, np.nan)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:35:02.178051Z","iopub.status.idle":"2025-02-05T12:35:02.178469Z","shell.execute_reply":"2025-02-05T12:35:02.178275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df = pd.read_csv(\"/kaggle/input/geo-ai-hack/test.csv\")\n\nsubmission_df[['prediction', 'filename']] = submission_df.apply(get_prediction_value, axis=1, result_type='expand')\nsubmission_df[[\"id\",\"prediction\"]].to_csv(\"hls_submission.csv\",index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:35:02.179068Z","iopub.status.idle":"2025-02-05T12:35:02.179488Z","shell.execute_reply":"2025-02-05T12:35:02.179303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ls kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T12:35:02.180243Z","iopub.status.idle":"2025-02-05T12:35:02.180631Z","shell.execute_reply":"2025-02-05T12:35:02.180467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}