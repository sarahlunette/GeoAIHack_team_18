{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T12:32:21.586787Z",
     "iopub.status.busy": "2025-02-05T12:32:21.586579Z",
     "iopub.status.idle": "2025-02-05T12:32:22.398971Z",
     "shell.execute_reply": "2025-02-05T12:32:22.397899Z",
     "shell.execute_reply.started": "2025-02-05T12:32:21.586767Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'InstaGeo-E2E-Geospatial-ML'...\n",
      "remote: Enumerating objects: 374, done.\u001b[K\n",
      "remote: Counting objects: 100% (105/105), done.\u001b[K\n",
      "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
      "remote: Total 374 (delta 80), reused 61 (delta 61), pack-reused 269 (from 1)\u001b[K\n",
      "Receiving objects: 100% (374/374), 1.42 MiB | 10.33 MiB/s, done.\n",
      "Resolving deltas: 100% (207/207), done.\n"
     ]
    }
   ],
   "source": [
    "# Clone the InstaGeo-E2E-Geospatial-ML repository from GitHub\n",
    "repository_url = \"https://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML\"\n",
    "!git clone {repository_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T12:32:22.400473Z",
     "iopub.status.busy": "2025-02-05T12:32:22.400125Z",
     "iopub.status.idle": "2025-02-05T12:32:22.622160Z",
     "shell.execute_reply": "2025-02-05T12:32:22.621333Z",
     "shell.execute_reply.started": "2025-02-05T12:32:22.400424Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd InstaGeo-E2E-Geospatial-ML\n",
    "git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T12:32:22.623424Z",
     "iopub.status.busy": "2025-02-05T12:32:22.623095Z",
     "iopub.status.idle": "2025-02-05T12:32:59.318926Z",
     "shell.execute_reply": "2025-02-05T12:32:59.318105Z",
     "shell.execute_reply.started": "2025-02-05T12:32:22.623393Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "# Navigate to the cloned InstaGeo-E2E-Geospatial-ML directory\n",
    "cd /kaggle/working/InstaGeo-E2E-Geospatial-ML\n",
    "# Stash any local changes to avoid conflicts when switching branches\n",
    "git stash\n",
    "#Switch to the 'geo-ai-hack' branch, which likely contains specific code for the Geo AI Hackathon\n",
    "git checkout geo-ai-hack\n",
    "# Install the InstaGeo package \n",
    "pip install -e .[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T12:32:59.320858Z",
     "iopub.status.busy": "2025-02-05T12:32:59.320609Z",
     "iopub.status.idle": "2025-02-05T12:33:00.423556Z",
     "shell.execute_reply": "2025-02-05T12:33:00.422671Z",
     "shell.execute_reply.started": "2025-02-05T12:32:59.320823Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pyproj import CRS, Transformer\n",
    "import rasterio\n",
    "os.environ[\"HYDRA_FULL_ERROR\"] =\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T12:33:00.427473Z",
     "iopub.status.busy": "2025-02-05T12:33:00.427239Z",
     "iopub.status.idle": "2025-02-05T12:33:00.434534Z",
     "shell.execute_reply": "2025-02-05T12:33:00.433692Z",
     "shell.execute_reply.started": "2025-02-05T12:33:00.427453Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def copy_last_25_percent_with_seg_maps(\n",
    "    input_chips_dir,\n",
    "    input_segmaps_dir,\n",
    "    output_chips_dir,\n",
    "    output_segmaps_dir\n",
    "):\n",
    "    \"\"\"\n",
    "    Copie les 25% de fichiers .tif \"depuis la fin\" du dossier `input_chips_dir`\n",
    "    vers `output_chips_dir`, et copie aussi les seg_maps correspondantes\n",
    "    (même nom, en remplaçant \"chip_\" par \"seg_map_\").\n",
    "\n",
    "    Exemple:\n",
    "      - input_chips_dir = \"/kaggle/input/hls_train/hls_train/chips\"\n",
    "      - input_segmaps_dir = \"/kaggle/input/hls_train/hls_train/seg_maps\"\n",
    "      - output_chips_dir = \"/kaggle/working/hls_train_subset/hls_train_subset/chips\"\n",
    "      - output_segmaps_dir = \"/kaggle/working/hls_train_subset/hls_train_subset/seg_maps\"\n",
    "    \"\"\"\n",
    "    input_chips_dir = Path(input_chips_dir)\n",
    "    input_segmaps_dir = Path(input_segmaps_dir)\n",
    "    output_chips_dir = Path(output_chips_dir)\n",
    "    output_segmaps_dir = Path(output_segmaps_dir)\n",
    "\n",
    "    output_chips_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_segmaps_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Lister tous les TIF dans chips\n",
    "    all_chip_files = [f for f in os.listdir(input_chips_dir) if f.endswith(\".tif\")]\n",
    "    all_chip_files.sort()  # Tri alphabétique (si votre naming est chronologique, ça marche)\n",
    "    total = len(all_chip_files)\n",
    "    if total == 0:\n",
    "        print(f\"[WARNING] Aucun TIFF dans {input_chips_dir}\")\n",
    "        return\n",
    "\n",
    "    # Calcul du nombre de fichiers à prendre (25%)\n",
    "    subset_count = int(total * 0.31)\n",
    "    # On prend les \"derniers\" subset_count\n",
    "    subset_chips = all_chip_files[-subset_count:]  # depuis la fin\n",
    "\n",
    "    print(f\"[INFO] Trouvé {total} TIFF dans {input_chips_dir}\")\n",
    "    print(f\"       On copie les {subset_count} derniers vers {output_chips_dir}\")\n",
    "\n",
    "    copied = 0\n",
    "    for chip_file in subset_chips:\n",
    "        src_chip = input_chips_dir / chip_file\n",
    "        dst_chip = output_chips_dir / chip_file\n",
    "        shutil.copy2(src_chip, dst_chip)\n",
    "        copied += 1\n",
    "\n",
    "        # Copier la seg_map correspondante si elle existe\n",
    "        seg_file = chip_file.replace(\"chip_\", \"seg_map_\")\n",
    "        seg_src = input_segmaps_dir / seg_file\n",
    "        if seg_src.exists():\n",
    "            dst_seg = output_segmaps_dir / seg_file\n",
    "            shutil.copy2(seg_src, dst_seg)\n",
    "\n",
    "    print(f\"[OK] {copied} fichiers copiés dans {output_chips_dir} + seg_maps associées.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T12:33:00.435983Z",
     "iopub.status.busy": "2025-02-05T12:33:00.435624Z",
     "iopub.status.idle": "2025-02-05T12:35:01.462259Z",
     "shell.execute_reply": "2025-02-05T12:35:01.461465Z",
     "shell.execute_reply.started": "2025-02-05T12:33:00.435950Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Trouvé 10428 TIFF dans /kaggle/input/geo-ai-hack/hls_train/hls_train/chips\n",
      "       On copie les 3232 derniers vers /kaggle/working/hls_train_subset/hls_train_subset/chips\n",
      "[OK] 3232 fichiers copiés dans /kaggle/working/hls_train_subset/hls_train_subset/chips + seg_maps associées.\n"
     ]
    }
   ],
   "source": [
    "copy_last_25_percent_with_seg_maps(\n",
    "    input_chips_dir   = \"/kaggle/input/geo-ai-hack/hls_train/hls_train/chips\",\n",
    "    input_segmaps_dir = \"/kaggle/input/geo-ai-hack/hls_train/hls_train/seg_maps\",\n",
    "    output_chips_dir  = \"/kaggle/working/hls_train_subset/hls_train_subset/chips\",\n",
    "    output_segmaps_dir= \"/kaggle/working/hls_train_subset/hls_train_subset/seg_maps\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T12:35:01.463507Z",
     "iopub.status.busy": "2025-02-05T12:35:01.463238Z",
     "iopub.status.idle": "2025-02-05T12:35:01.471354Z",
     "shell.execute_reply": "2025-02-05T12:35:01.470538Z",
     "shell.execute_reply.started": "2025-02-05T12:35:01.463476Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from pathlib import Path\n",
    "\n",
    "def replace_swir2_with_ndwi_inplace_multitime_no_spike(chips_dir):\n",
    "    \"\"\"\n",
    "    Pour chaque TIFF dans chips_dir (shape [18, H, W] = 3 time steps × 6 bandes):\n",
    "      - Lecture en mémoire, suppression immédiate du fichier\n",
    "      - Calcul NDWI pour chaque time step:\n",
    "         NDWI = (NIR - SWIR1) / (NIR + SWIR1 + 1e-6)\n",
    "      - On remplace la bande SWIR2 (index 5) par NDWI\n",
    "      - On réécrit au même chemin un TIFF de shape [18, H, W],\n",
    "        où la bande 5 (SWIR2) est devenue NDWI.\n",
    "\n",
    "    Structure initiale par time step (6 bandes):\n",
    "       0 = Blue\n",
    "       1 = Green\n",
    "       2 = Red\n",
    "       3 = NIR\n",
    "       4 = SWIR1\n",
    "       5 = SWIR2  --> on va la remplacer par NDWI\n",
    "    \"\"\"\n",
    "\n",
    "    chips_dir = Path(chips_dir)\n",
    "    for chip_file in os.listdir(chips_dir):\n",
    "        if not chip_file.endswith(\".tif\"):\n",
    "            continue\n",
    "\n",
    "        chip_path = chips_dir / chip_file\n",
    "\n",
    "        # 1) Lecture en RAM\n",
    "        with rasterio.open(chip_path) as src:\n",
    "            array = src.read().astype(np.float32)  # shape (18, H, W)\n",
    "            profile = src.profile\n",
    "\n",
    "        # 2) Supprime aussitôt le TIFF original (libère l'espace)\n",
    "        os.remove(chip_path)\n",
    "\n",
    "        # 3) Vérifions qu'on a 18 canaux (3 time steps × 6 bandes)\n",
    "        if array.shape[0] != 18:\n",
    "            print(f\"[SKIP] {chip_file}: {array.shape[0]} canaux (pas 18).\")\n",
    "            # On réécrit le fichier d'origine pour ne pas le perdre\n",
    "            with rasterio.open(chip_path, \"w\", **profile) as dst:\n",
    "                dst.write(array)\n",
    "            continue\n",
    "\n",
    "        time_steps = 3\n",
    "        bands_per_ts = 6\n",
    "\n",
    "        # 4) Calcul NDWI pour chacun des 3 pas de temps\n",
    "        for t in range(time_steps):\n",
    "            offset_in = t * bands_per_ts\n",
    "\n",
    "            # Indices: \n",
    "            # 3 = NIR, 4 = SWIR1, 5 = SWIR2 (à remplacer)\n",
    "            nir   = array[offset_in + 3]\n",
    "            swir1 = array[offset_in + 4]\n",
    "\n",
    "            ndwi = (nir - swir1) / (nir + swir1 + 1e-6)\n",
    "\n",
    "            # Remplace la bande SWIR2 (index 5) par NDWI\n",
    "            array[offset_in + 5] = ndwi\n",
    "\n",
    "        # 5) On garde la même shape (18 canaux)\n",
    "        profile.update(dtype=\"float32\", count=18)\n",
    "\n",
    "        # 6) Écrit le nouveau TIFF\n",
    "        with rasterio.open(chip_path, \"w\", **profile) as dst:\n",
    "            dst.write(array)\n",
    "\n",
    "    print(f\"[OK] SWIR2 remplacé par NDWI (in-place, 18 canaux) dans : {chips_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T12:37:59.586673Z",
     "iopub.status.busy": "2025-02-05T12:37:59.586308Z",
     "iopub.status.idle": "2025-02-05T12:39:28.911011Z",
     "shell.execute_reply": "2025-02-05T12:39:28.910072Z",
     "shell.execute_reply.started": "2025-02-05T12:37:59.586643Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] SWIR2 remplacé par NDWI (in-place, 18 canaux) dans : /kaggle/working/hls_train_subset/hls_train_subset/chips\n"
     ]
    }
   ],
   "source": [
    "replace_swir2_with_ndwi_inplace_multitime_no_spike(\"/kaggle/working/hls_train_subset/hls_train_subset/chips\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T12:39:28.912609Z",
     "iopub.status.busy": "2025-02-05T12:39:28.912300Z",
     "iopub.status.idle": "2025-02-05T12:39:28.945558Z",
     "shell.execute_reply": "2025-02-05T12:39:28.944937Z",
     "shell.execute_reply.started": "2025-02-05T12:39:28.912578Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array.shape : (18, 256, 256)\n",
      "src.count  : 18\n",
      "src.height : 256\n",
      "src.width  : 256\n"
     ]
    }
   ],
   "source": [
    "chips_dir = \"/kaggle/working/hls_train_subset/hls_train_subset/chips\"  # adaptez le chemin à votre cas\n",
    "\n",
    "# Lister tous les fichiers TIFF du dossier\n",
    "tif_files = [f for f in os.listdir(chips_dir) if f.endswith(\".tif\")]\n",
    "\n",
    "if tif_files:\n",
    "    first_tif = tif_files[0]\n",
    "    first_tif_path = os.path.join(chips_dir, first_tif)\n",
    "\n",
    "    with rasterio.open(first_tif_path) as src:\n",
    "        # Méthode 1: lire la totalité des canaux en mémoire\n",
    "        array = src.read()  \n",
    "        # array.shape renvoie (nombre_canaux, hauteur, largeur)\n",
    "        print(\"array.shape :\", array.shape)\n",
    "\n",
    "        # Méthode 2: accéder directement aux attributs\n",
    "        print(\"src.count  :\", src.count)   # nombre de canaux\n",
    "        print(\"src.height :\", src.height)\n",
    "        print(\"src.width  :\", src.width)\n",
    "\n",
    "else:\n",
    "    print(\"Aucun fichier .tif trouvé dans\", chips_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T12:39:28.947526Z",
     "iopub.status.busy": "2025-02-05T12:39:28.947299Z",
     "iopub.status.idle": "2025-02-05T12:39:28.952935Z",
     "shell.execute_reply": "2025-02-05T12:39:28.952122Z",
     "shell.execute_reply.started": "2025-02-05T12:39:28.947507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_label_mapping(root_dir, input_subdir, output_csv):\n",
    "    \"\"\"\n",
    "    Generate a CSV mapping input chips to corresponding segmentation maps.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str or Path): Root directory containing the subdirectories for chips and segmentation maps.\n",
    "        input_subdir (str): Subdirectory path for chips within the root directory.\n",
    "        output_csv (str or Path): Output path for the generated CSV file.\n",
    "    \"\"\"\n",
    "    root_dir = Path(root_dir)\n",
    "    chips_orig = os.listdir(root_dir / input_subdir / \"chips\")\n",
    "    if os.path.exists(root_dir / input_subdir / \"seg_maps\"):\n",
    "        add_label = True\n",
    "    else:\n",
    "        add_label = False\n",
    "\n",
    "    chips = [chip.replace(\"chip\", f\"{input_subdir}/chips/chip\") for chip in chips_orig]\n",
    "\n",
    "    if add_label:\n",
    "        seg_maps = [chip.replace(\"chip\", f\"{input_subdir}/seg_maps/seg_map\") for chip in chips_orig]\n",
    "        df = pd.DataFrame({\"Input\": chips, \"Label\": seg_maps})\n",
    "    else:\n",
    "        df = pd.DataFrame({\"Input\": chips})\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    print(f\"Number of rows is: {df.shape[0]}\")\n",
    "    print(f\"CSV generated and saved to: {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T12:39:28.954381Z",
     "iopub.status.busy": "2025-02-05T12:39:28.954147Z",
     "iopub.status.idle": "2025-02-05T12:39:28.995580Z",
     "shell.execute_reply": "2025-02-05T12:39:28.994940Z",
     "shell.execute_reply.started": "2025-02-05T12:39:28.954357Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows is: 3232\n",
      "CSV generated and saved to: train_ds_subset.csv\n"
     ]
    }
   ],
   "source": [
    "# Par exemple, on a transformé /kaggle/working/hls_train/hls_train/chips en 8 bandes\n",
    "\n",
    "root_dir = \"/kaggle/working\"  # Le dossier principal\n",
    "generate_label_mapping(\n",
    "    root_dir=root_dir,\n",
    "    input_subdir=\"hls_train_subset/hls_train_subset\",  # Chemin vers vos chips \n",
    "    output_csv=\"train_ds_subset.csv\"\n",
    ")\n",
    "\n",
    "#il faudra ajouter le test ensuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T12:39:28.996540Z",
     "iopub.status.busy": "2025-02-05T12:39:28.996318Z",
     "iopub.status.idle": "2025-02-05T12:39:29.001006Z",
     "shell.execute_reply": "2025-02-05T12:39:29.000204Z",
     "shell.execute_reply.started": "2025-02-05T12:39:28.996521Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_validation_data(mapping_csv, validation_split=0.3):\n",
    "    \"\"\"\n",
    "    Split data into training and validation sets based on a CSV file mapping `chips` and `seg_maps`.\n",
    "\n",
    "    Args:\n",
    "        mapping_csv (str or Path): Path to the CSV file containing the mapping between `chips` and `seg_maps`.\n",
    "        data_dir (str or Path): Path to the merged directory containing all files.\n",
    "        validation_dir (str or Path): Path to the new directory for validation files.\n",
    "        validation_split (float): Fraction of the data to use as the validation set.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(mapping_csv)\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    num_val = int(len(df) * validation_split)\n",
    "    train_df = df[num_val:]\n",
    "    val_df = df[:num_val]\n",
    "    train_df.to_csv(\"train_split.csv\",index=False)    \n",
    "    print(f\"CSV train split  saved to: train_split.csv\")\n",
    "    val_df.to_csv(\"validation_split.csv\",index=False)    \n",
    "    print(f\"CSV validation split  saved to: validation_split.csv\")\n",
    "    \n",
    "    return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T12:39:29.002117Z",
     "iopub.status.busy": "2025-02-05T12:39:29.001854Z",
     "iopub.status.idle": "2025-02-05T12:39:29.052271Z",
     "shell.execute_reply": "2025-02-05T12:39:29.051651Z",
     "shell.execute_reply.started": "2025-02-05T12:39:29.002086Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV train split  saved to: train_split.csv\n",
      "CSV validation split  saved to: validation_split.csv\n"
     ]
    }
   ],
   "source": [
    "split_validation_data(\"train_ds_subset.csv\", validation_split=0.3)\n",
    "# Cela va créer train_split.csv et validation_split.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T12:39:29.053344Z",
     "iopub.status.busy": "2025-02-05T12:39:29.053053Z",
     "iopub.status.idle": "2025-02-05T12:39:29.061819Z",
     "shell.execute_reply": "2025-02-05T12:39:29.061095Z",
     "shell.execute_reply.started": "2025-02-05T12:39:29.053316Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "#on vérifie que notre image a le bon nombre de bandes \n",
    "\n",
    "with rasterio.open(\"hls_train_subset/hls_train_subset/chips/chip_20201201_S30_T37QGB_2020304T074041_3_4.tif\") as src : \n",
    "    print(src.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T12:39:29.063831Z",
     "iopub.status.busy": "2025-02-05T12:39:29.063583Z",
     "iopub.status.idle": "2025-02-05T12:41:47.435165Z",
     "shell.execute_reply": "2025-02-05T12:41:47.434297Z",
     "shell.execute_reply.started": "2025-02-05T12:39:29.063803Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-05 12:39:44,080][__main__][INFO] - Script: /kaggle/working/InstaGeo-E2E-Geospatial-ML/instageo/model/run.py\n",
      "[2025-02-05 12:39:44,083][__main__][INFO] - Imported hydra config:\n",
      "root_dir: /kaggle/working\n",
      "valid_filepath: null\n",
      "train_filepath: train_ds_subset.csv\n",
      "test_filepath: null\n",
      "checkpoint_path: null\n",
      "output_dir: null\n",
      "mode: stats\n",
      "train:\n",
      "  learning_rate: 0.0001\n",
      "  num_epochs: 1\n",
      "  batch_size: 8\n",
      "  class_weights:\n",
      "  - 1\n",
      "  - 1\n",
      "  ignore_index: -1\n",
      "  weight_decay: 0.1\n",
      "model:\n",
      "  freeze_backbone: false\n",
      "  num_classes: 2\n",
      "dataloader:\n",
      "  bands:\n",
      "  - 0\n",
      "  - 1\n",
      "  - 2\n",
      "  - 3\n",
      "  - 4\n",
      "  - 5\n",
      "  - 6\n",
      "  - 7\n",
      "  - 8\n",
      "  - 9\n",
      "  - 10\n",
      "  - 11\n",
      "  - 12\n",
      "  - 13\n",
      "  - 14\n",
      "  - 15\n",
      "  - 16\n",
      "  - 17\n",
      "  mean:\n",
      "  - 623.2724609375\n",
      "  - 1247.657958984375\n",
      "  - 1772.24169921875\n",
      "  - 2371.256103515625\n",
      "  - 2862.867431640625\n",
      "  - 2357.759765625\n",
      "  std:\n",
      "  - 2182.050048828125\n",
      "  - 2248.420654296875\n",
      "  - 2302.53515625\n",
      "  - 2372.204345703125\n",
      "  - 2398.52685546875\n",
      "  - 2292.96435546875\n",
      "  img_size: 256\n",
      "  temporal_dim: 3\n",
      "  replace_label:\n",
      "  - -9999\n",
      "  - -1\n",
      "  reduce_to_zero: false\n",
      "  no_data_value: -9999\n",
      "  constant_multiplier: 1.0\n",
      "test:\n",
      "  img_size: 256\n",
      "  crop_size: 256\n",
      "  stride: 256\n",
      "  mask_cloud: false\n",
      "\n",
      "[824.8154907226562, 1380.176513671875, 1866.7628173828125, 2610.20849609375, 3014.04541015625, -60.25557327270508]\n",
      "[2098.12451171875, 2137.667724609375, 2160.350830078125, 2211.607666015625, 2191.01220703125, 139875.90625]\n"
     ]
    }
   ],
   "source": [
    "!python -m instageo.model.run \\\n",
    "    --config-name=locust \\\n",
    "    root_dir=\"/kaggle/working\" \\\n",
    "    train_filepath=\"train_ds_subset.csv\" \\\n",
    "    dataloader.temporal_dim=3 \\\n",
    "    train.batch_size=8 \\\n",
    "    train.num_epochs=1 \\\n",
    "    mode=stats \\\n",
    "    dataloader.bands=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]\\\n",
    "    dataloader.mean=[623.2724609375,1247.657958984375,1772.24169921875,2371.256103515625,2862.867431640625,2357.759765625]\\\n",
    "    dataloader.std=[2182.050048828125,2248.420654296875,2302.53515625,2372.204345703125,2398.52685546875,2292.96435546875]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T12:43:17.579619Z",
     "iopub.status.busy": "2025-02-05T12:43:17.579257Z",
     "iopub.status.idle": "2025-02-05T12:43:17.584667Z",
     "shell.execute_reply": "2025-02-05T12:43:17.583877Z",
     "shell.execute_reply.started": "2025-02-05T12:43:17.579595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_yml(filepath):\n",
    "    \"\"\"Load data from a YAML file.\n",
    "\n",
    "    Args:\n",
    "        filepath (str | Path): The path to the YAML file.\n",
    "\n",
    "    Returns:\n",
    "        Dict: The loaded data, or None if the file does not exist.\n",
    "    \"\"\"\n",
    "    filepath=Path(filepath)\n",
    "    with filepath.open() as f:\n",
    "        return yaml.safe_load(f)\n",
    "        \n",
    "def save_yml(data,filepath):\n",
    "    \"\"\"Save data to a YAML file.\n",
    "\n",
    "    Args:\n",
    "        data (Dict): The data to save.\n",
    "        filepath (str | Path): The file path to save the YAML to.\n",
    "    \"\"\"\n",
    "    filepath = Path(filepath)\n",
    "    with filepath.open(\"w\") as f:\n",
    "        yaml.dump(data, f)\n",
    "    print(f\"Data saved to {filepath}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T12:43:17.997728Z",
     "iopub.status.busy": "2025-02-05T12:43:17.997483Z",
     "iopub.status.idle": "2025-02-05T12:43:18.012492Z",
     "shell.execute_reply": "2025-02-05T12:43:18.011748Z",
     "shell.execute_reply.started": "2025-02-05T12:43:17.997708Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to InstaGeo-E2E-Geospatial-ML/instageo/model/configs/locust.yaml.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "locust_cfg_path = \"InstaGeo-E2E-Geospatial-ML/instageo/model/configs/locust.yaml\"\n",
    "locust_cfg = load_yml(locust_cfg_path)\n",
    "\n",
    "# Recopiez les valeurs mean/std découvertes dans la sortie ci-dessus\n",
    "locust_cfg[\"mean\"] = [867.623046875, 1439.7684326171875, 1951.5145263671875, 2704.58984375, 3148.905029296875, -60.25557327270508]  \n",
    "locust_cfg[\"std\"]  = [1995.3145751953125, 2033.8626708984375, 2060.29931640625, 2110.346923828125, 2111.359619140625, 139875.90625]\n",
    "\n",
    "# Si besoin, configurer in_channels=8 (selon la version d'InstaGeo)\n",
    "#locust_cfg[\"model\"][\"in_channels\"] = 8\n",
    "\n",
    "save_yml(locust_cfg, locust_cfg_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T12:43:21.108650Z",
     "iopub.status.busy": "2025-02-05T12:43:21.108324Z",
     "iopub.status.idle": "2025-02-05T12:43:28.902035Z",
     "shell.execute_reply": "2025-02-05T12:43:28.901202Z",
     "shell.execute_reply.started": "2025-02-05T12:43:21.108620Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résumé des shapes rencontrées (nombre de canaux) :\n",
      " - 18 canaux : 3232 fichier(s)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import rasterio\n",
    "import os\n",
    "\n",
    "df = pd.read_csv(\"train_ds_subset.csv\")\n",
    "\n",
    "root_dir = \"/kaggle/working\"  # adaptez si nécessaire\n",
    "shapes_count = {}\n",
    "\n",
    "for path_rel in df[\"Input\"]:\n",
    "    path_abs = os.path.join(root_dir, path_rel)\n",
    "    if not os.path.exists(path_abs):\n",
    "        print(f\"[WARNING] Fichier introuvable : {path_abs}\")\n",
    "        continue\n",
    "    with rasterio.open(path_abs) as src:\n",
    "        band_count = src.count\n",
    "    shapes_count[band_count] = shapes_count.get(band_count, 0) + 1\n",
    "\n",
    "print(\"Résumé des shapes rencontrées (nombre de canaux) :\")\n",
    "for band_count, nb_files in shapes_count.items():\n",
    "    print(f\" - {band_count} canaux : {nb_files} fichier(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T12:43:52.605792Z",
     "iopub.status.busy": "2025-02-05T12:43:52.605481Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-05 12:43:58,970][__main__][INFO] - Script: /kaggle/working/InstaGeo-E2E-Geospatial-ML/instageo/model/run.py\n",
      "[2025-02-05 12:43:58,974][__main__][INFO] - Imported hydra config:\n",
      "checkpoint_path: null\n",
      "dataloader:\n",
      "  bands:\n",
      "  - 0\n",
      "  - 1\n",
      "  - 2\n",
      "  - 3\n",
      "  - 4\n",
      "  - 5\n",
      "  - 6\n",
      "  - 7\n",
      "  - 8\n",
      "  - 9\n",
      "  - 10\n",
      "  - 11\n",
      "  - 12\n",
      "  - 13\n",
      "  - 14\n",
      "  - 15\n",
      "  - 16\n",
      "  - 17\n",
      "  constant_multiplier: 1.0\n",
      "  img_size: 256\n",
      "  mean:\n",
      "  - 867.623046875\n",
      "  - 1439.7684326171875\n",
      "  - 1951.5145263671875\n",
      "  - 2704.58984375\n",
      "  - 3148.905029296875\n",
      "  - -60.25557327270508\n",
      "  no_data_value: -9999\n",
      "  reduce_to_zero: false\n",
      "  replace_label:\n",
      "  - -9999\n",
      "  - -1\n",
      "  std:\n",
      "  - 1995.3145751953125\n",
      "  - 2033.8626708984375\n",
      "  - 2060.29931640625\n",
      "  - 2110.346923828125\n",
      "  - 2111.359619140625\n",
      "  - 139875.90625\n",
      "  temporal_dim: 3\n",
      "mean:\n",
      "- 867.623046875\n",
      "- 1439.7684326171875\n",
      "- 1951.5145263671875\n",
      "- 2704.58984375\n",
      "- 3148.905029296875\n",
      "- -60.25557327270508\n",
      "mode: train\n",
      "model:\n",
      "  freeze_backbone: false\n",
      "  num_classes: 2\n",
      "output_dir: null\n",
      "root_dir: /kaggle/working\n",
      "std:\n",
      "- 1995.3145751953125\n",
      "- 2033.8626708984375\n",
      "- 2060.29931640625\n",
      "- 2110.346923828125\n",
      "- 2111.359619140625\n",
      "- 139875.90625\n",
      "test:\n",
      "  crop_size: 256\n",
      "  img_size: 256\n",
      "  mask_cloud: false\n",
      "  stride: 256\n",
      "test_filepath: null\n",
      "train:\n",
      "  batch_size: 8\n",
      "  class_weights:\n",
      "  - 1\n",
      "  - 1\n",
      "  ignore_index: -1\n",
      "  learning_rate: 0.0001\n",
      "  num_epochs: 10\n",
      "  weight_decay: 0.1\n",
      "train_filepath: train_ds_subset.csv\n",
      "valid_filepath: validation_split.csv\n",
      "\n",
      "[2025-02-05 12:44:12,608][absl][INFO] - Download successful on attempt 1\n",
      "[2025-02-05 12:44:12,792][absl][INFO] - Download successful on attempt 1\n",
      "/kaggle/working/InstaGeo-E2E-Geospatial-ML/instageo/model/model.py:155: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(weights_path, map_location=\"cpu\")\n",
      "[2025-02-05 12:44:14,871][root][INFO] - GPU is available. Using GPU...\n",
      "[2025-02-05 12:44:14,909][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True\n",
      "[2025-02-05 12:44:14,910][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores\n",
      "[2025-02-05 12:44:14,910][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs\n",
      "[2025-02-05 12:44:14,968][lightning_fabric.utilities.distributed][INFO] - Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "[2025-02-05 12:44:21,082][__main__][INFO] - Script: /kaggle/working/InstaGeo-E2E-Geospatial-ML/instageo/model/run.py\n",
      "[2025-02-05 12:44:21,086][__main__][INFO] - Imported hydra config:\n",
      "checkpoint_path: null\n",
      "dataloader:\n",
      "  bands:\n",
      "  - 0\n",
      "  - 1\n",
      "  - 2\n",
      "  - 3\n",
      "  - 4\n",
      "  - 5\n",
      "  - 6\n",
      "  - 7\n",
      "  - 8\n",
      "  - 9\n",
      "  - 10\n",
      "  - 11\n",
      "  - 12\n",
      "  - 13\n",
      "  - 14\n",
      "  - 15\n",
      "  - 16\n",
      "  - 17\n",
      "  constant_multiplier: 1.0\n",
      "  img_size: 256\n",
      "  mean:\n",
      "  - 867.623046875\n",
      "  - 1439.7684326171875\n",
      "  - 1951.5145263671875\n",
      "  - 2704.58984375\n",
      "  - 3148.905029296875\n",
      "  - -60.25557327270508\n",
      "  no_data_value: -9999\n",
      "  reduce_to_zero: false\n",
      "  replace_label:\n",
      "  - -9999\n",
      "  - -1\n",
      "  std:\n",
      "  - 1995.3145751953125\n",
      "  - 2033.8626708984375\n",
      "  - 2060.29931640625\n",
      "  - 2110.346923828125\n",
      "  - 2111.359619140625\n",
      "  - 139875.90625\n",
      "  temporal_dim: 3\n",
      "mean:\n",
      "- 867.623046875\n",
      "- 1439.7684326171875\n",
      "- 1951.5145263671875\n",
      "- 2704.58984375\n",
      "- 3148.905029296875\n",
      "- -60.25557327270508\n",
      "mode: train\n",
      "model:\n",
      "  freeze_backbone: false\n",
      "  num_classes: 2\n",
      "output_dir: null\n",
      "root_dir: /kaggle/working\n",
      "std:\n",
      "- 1995.3145751953125\n",
      "- 2033.8626708984375\n",
      "- 2060.29931640625\n",
      "- 2110.346923828125\n",
      "- 2111.359619140625\n",
      "- 139875.90625\n",
      "test:\n",
      "  crop_size: 256\n",
      "  img_size: 256\n",
      "  mask_cloud: false\n",
      "  stride: 256\n",
      "test_filepath: null\n",
      "train:\n",
      "  batch_size: 8\n",
      "  class_weights:\n",
      "  - 1\n",
      "  - 1\n",
      "  ignore_index: -1\n",
      "  learning_rate: 0.0001\n",
      "  num_epochs: 10\n",
      "  weight_decay: 0.1\n",
      "train_filepath: train_ds_subset.csv\n",
      "valid_filepath: validation_split.csv\n",
      "\n",
      "[2025-02-05 12:44:31,922][absl][INFO] - File '/root/.instageo/prithvi/Prithvi_EO_V1_100M.pt' already exists. Skipping download.\n",
      "[2025-02-05 12:44:31,922][absl][INFO] - File '/root/.instageo/prithvi/config.yaml' already exists. Skipping download.\n",
      "/kaggle/working/InstaGeo-E2E-Geospatial-ML/instageo/model/model.py:155: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(weights_path, map_location=\"cpu\")\n",
      "[2025-02-05 12:44:34,372][root][INFO] - GPU is available. Using GPU...\n",
      "[2025-02-05 12:44:34,540][lightning_fabric.utilities.distributed][INFO] - Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "[2025-02-05 12:44:34,545][pytorch_lightning.utilities.rank_zero][INFO] - ----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "2025-02-05 12:44:36.714874: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-05 12:44:36.893116: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-05 12:44:36.943607: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working/outputs/first_run exists and is not empty.\n",
      "[2025-02-05 12:44:46,786][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[2025-02-05 12:44:46,786][pytorch_lightning.accelerators.cuda][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[2025-02-05 12:44:47,027][pytorch_lightning.callbacks.model_summary][INFO] - \n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | net       | PrithviSeg       | 134 M  | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "133 M     Trainable params\n",
      "590 K     Non-trainable params\n",
      "134 M     Total params\n",
      "537.703   Total estimated model params size (MB)\n",
      "198       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Sanity Checking DataLoader 0: 100%|███████████████| 2/2 [00:02<00:00,  0.87it/s]/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_aAcc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_mIoU', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_IoU_0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_IoU_1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_Acc_0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_Acc_1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_Precision_0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_Precision_1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_Recall_0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_Recall_1', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "Epoch 0:  33%|▎| 67/202 [02:35<05:12,  0.43it/s, v_num=0, train_loss_step=0.673,"
     ]
    }
   ],
   "source": [
    "# Train the InstaGeo model using the Locust configuration\n",
    "!python -m instageo.model.run  --config-name=locust \\\n",
    "    hydra.run.dir=\"/kaggle/working/outputs/first_run\" \\\n",
    "    root_dir=\"/kaggle/working\" \\\n",
    "    train.batch_size=8 \\\n",
    "    train.num_epochs=10\\\n",
    "    mode=train \\\n",
    "    dataloader.mean=\"[867.623046875, 1439.7684326171875, 1951.5145263671875, 2704.58984375, 3148.905029296875, -60.25557327270508]\"\\\n",
    "    dataloader.std=\"[1995.3145751953125, 2033.8626708984375, 2060.29931640625, 2110.346923828125, 2111.359619140625, 139875.90625]\"\\\n",
    "    train_filepath=\"train_ds_subset.csv\" \\\n",
    "    valid_filepath=\"validation_split.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-05T12:35:02.174683Z",
     "iopub.status.idle": "2025-02-05T12:35:02.175120Z",
     "shell.execute_reply": "2025-02-05T12:35:02.174940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generate_label_mapping('/kaggle/input/geo-ai-hack', 'hls_test/hls_test', \"test_ds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-05T12:35:02.175889Z",
     "iopub.status.idle": "2025-02-05T12:35:02.176258Z",
     "shell.execute_reply": "2025-02-05T12:35:02.176100Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python -m instageo.model.run --config-name=locust \\\n",
    "    root_dir=\"/kaggle/input/geo-ai-hack\" \\\n",
    "    test_filepath=\"validation_split.csv\" \\\n",
    "    train.batch_size=16 \\\n",
    "    checkpoint_path='/kaggle/working/outputs/first_run/instageo_best_checkpoint.ckpt' \\\n",
    "    mode=eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-05T12:35:02.176807Z",
     "iopub.status.idle": "2025-02-05T12:35:02.177208Z",
     "shell.execute_reply": "2025-02-05T12:35:02.177040Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python -m instageo.model.run --config-name=locust \\\n",
    "    root_dir=\"/kaggle/input/geo-ai-hack\" \\\n",
    "    test_filepath=\"test_ds.csv\" \\\n",
    "    train.batch_size=16 \\\n",
    "    checkpoint_path='/kaggle/working/outputs/first_run/instageo_best_checkpoint.ckpt' \\\n",
    "    output_dir='/kaggle/working/predictions' \\\n",
    "    mode=chip_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-05T12:35:02.178051Z",
     "iopub.status.idle": "2025-02-05T12:35:02.178469Z",
     "shell.execute_reply": "2025-02-05T12:35:02.178275Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "predictions_directory = \"/kaggle/working/predictions\"\n",
    "prediction_files = os.listdir(predictions_directory)\n",
    "\n",
    "def get_prediction_value(row):\n",
    "    matching_files = [f for f in prediction_files if (str(row['date']) in f) and (row['mgrs_tile_id'] in f)]\n",
    "    if not matching_files:\n",
    "        return (np.nan, np.nan)\n",
    "    for file in matching_files:\n",
    "        with rasterio.open(f\"{predictions_directory}/{file}\") as src:\n",
    "            width, height = src.width, src.height\n",
    "            affine_transform = rasterio.transform.AffineTransformer(src.transform)\n",
    "            transformer = Transformer.from_crs(CRS.from_epsg(4326), src.crs, always_xy=True)\n",
    "            x_chip, y_chip = transformer.transform(row['x'], row['y'])\n",
    "            x_offset, y_offset = affine_transform.rowcol(x_chip, y_chip)\n",
    "            \n",
    "            if 0 <= x_offset < width and 0 <= y_offset < height:\n",
    "                return src.read(1)[y_offset, x_offset], file\n",
    "    return (np.nan, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-05T12:35:02.179068Z",
     "iopub.status.idle": "2025-02-05T12:35:02.179488Z",
     "shell.execute_reply": "2025-02-05T12:35:02.179303Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv(\"/kaggle/input/geo-ai-hack/test.csv\")\n",
    "\n",
    "submission_df[['prediction', 'filename']] = submission_df.apply(get_prediction_value, axis=1, result_type='expand')\n",
    "submission_df[[\"id\",\"prediction\"]].to_csv(\"hls_submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-05T12:35:02.180243Z",
     "iopub.status.idle": "2025-02-05T12:35:02.180631Z",
     "shell.execute_reply": "2025-02-05T12:35:02.180467Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ls kaggle/working"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11007828,
     "sourceId": 91683,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
