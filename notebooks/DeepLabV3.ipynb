{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os\n",
        "os.makedirs('/content/drive/MyDrive/geo/',exist_ok=True)\n",
        "\n",
        "import os\n",
        "os.makedirs('/content/drive/MyDrive/geo/predictions_rcnn/',exist_ok=True)\n",
        "\n",
        "# Import\n",
        "!pip install rasterio\n",
        "from sklearn.metrics import precision_recall_curve, auc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTpdwFR8Mrhe",
        "outputId": "62ddf344-1507-467f-fced-aa7da6e1d48c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting rasterio\n",
            "  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2024.12.14)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.1.8)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (1.26.4)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.1)\n",
            "Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Installing collected packages: cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import rasterio\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "\n",
        "# Custom Dataset Class for 8-Channel Segmentation\n",
        "class CustomSegmentationDataset(Dataset):\n",
        "    def __init__(self, image_paths, mask_paths=None, transform=None, is_test=False):\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths if not is_test else None\n",
        "        self.transform = transform\n",
        "        self.is_test = is_test\n",
        "        self.samples = []\n",
        "\n",
        "        if not is_test:\n",
        "            for img_path, mask_path in zip(image_paths, mask_paths):\n",
        "                if os.path.exists(img_path) and os.path.exists(mask_path):\n",
        "                    self.samples.append((img_path, mask_path))\n",
        "        else:\n",
        "            for img_path in image_paths:\n",
        "                if os.path.exists(img_path):\n",
        "                    self.samples.append((img_path, None))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path, mask_path = self.samples[idx]\n",
        "        preprocessing = Preprocessing(image_path)\n",
        "        image = preprocessing.preprocess_image()\n",
        "        image = torch.tensor(image.transpose(2, 0, 1), dtype=torch.float32)  # (C, H, W)\n",
        "\n",
        "        if self.is_test:\n",
        "            return image, {\"image_id\": torch.tensor([idx])}\n",
        "        else:\n",
        "            mask = plt.imread(mask_path)\n",
        "            mask = torch.tensor(mask, dtype=torch.long).squeeze(0)  # Ensure it's (H, W)\n",
        "\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            return image, mask\n",
        "\n",
        "# Preprocessing Class for 8 Channels\n",
        "class Preprocessing:\n",
        "    def __init__(self, image_path):\n",
        "        self.image_path = image_path\n",
        "\n",
        "    def load_bands(self):\n",
        "        with rasterio.open(self.image_path) as src:\n",
        "            blue = src.read(1)\n",
        "            green = src.read(2)\n",
        "            red = src.read(3)\n",
        "            nir = src.read(4)\n",
        "            swir1 = src.read(5)\n",
        "            swir2 = src.read(6)\n",
        "        return blue, green, red, nir, swir1, swir2\n",
        "\n",
        "    def preprocess_image(self):\n",
        "        blue, green, red, nir, swir1, swir2 = self.load_bands()\n",
        "        ndvi = self.compute_ndvi(red, nir)\n",
        "        evi = self.compute_evi(nir, red, blue)\n",
        "        normalized_bands = [self.normalize_band(band) for band in [blue, green, red, nir, swir1, swir2]]\n",
        "        image = np.stack(normalized_bands + [ndvi, evi], axis=-1)  # Stack 8 channels\n",
        "        return image\n",
        "\n",
        "    def normalize_band(self, band):\n",
        "        return (band - np.min(band)) / (np.max(band) - np.min(band))\n",
        "\n",
        "    def compute_ndvi(self, red, nir):\n",
        "        return (nir - red) / (nir + red + 1e-6)\n",
        "\n",
        "    def compute_evi(self, nir, red, blue, g=2.5, c1=6, c2=7.5, l=1):\n",
        "        return np.clip(g * (nir - red) / (nir + c1 * red - c2 * blue + l), 0, 1)\n",
        "\n",
        "# Modified DeepLabV3 for 8 Channels\n",
        "class DeepLabV3Model:\n",
        "    def __init__(self, num_classes=2, device='cuda'):\n",
        "        self.device = device\n",
        "        self.model = deeplabv3_resnet50(pretrained=True)\n",
        "\n",
        "        # Modify input layer to accept 8 channels\n",
        "        in_features = self.model.backbone.conv1.in_channels\n",
        "        self.model.backbone.conv1 = nn.Conv2d(8, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "        # Modify output layer for segmentation classes\n",
        "        self.model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "\n",
        "    def train(self, dataloader, num_epochs=10):\n",
        "        self.model.train()\n",
        "        for epoch in range(num_epochs):\n",
        "            running_loss = 0.0\n",
        "            for images, masks in dataloader:\n",
        "                images, masks = images.to(self.device), masks.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(images)['out']\n",
        "                loss = self.criterion(outputs, masks)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(dataloader):.4f}\")\n",
        "\n",
        "    def evaluate(self, dataloader):\n",
        "        self.model.eval()\n",
        "        iou_scores = []\n",
        "        with torch.no_grad():\n",
        "            for images, masks in dataloader:\n",
        "                images, masks = images.to(self.device), masks.to(self.device)\n",
        "                outputs = self.model(images)['out']\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                intersection = (preds & masks).float().sum()\n",
        "                union = (preds | masks).float().sum()\n",
        "                iou_scores.append(intersection / union)\n",
        "        mean_iou = sum(iou_scores) / len(iou_scores)\n",
        "        print(f\"Mean IoU: {mean_iou:.4f}\")\n",
        "\n",
        "# Load dataset\n",
        "train_csv = pd.read_csv(\"train_ds.csv\")\n",
        "test_csv = pd.read_csv(\"test_ds.csv\")\n",
        "\n",
        "train_image_paths = train_csv[\"Input\"].tolist()\n",
        "train_mask_paths = train_csv[\"Label\"].tolist()\n",
        "test_image_paths = test_csv[\"Input\"].tolist()\n",
        "\n",
        "# Data Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "])\n",
        "\n",
        "# Create Datasets and Dataloaders\n",
        "train_dataset = CustomSegmentationDataset(image_paths=train_image_paths, mask_paths=train_mask_paths, transform=transform)\n",
        "test_dataset = CustomSegmentationDataset(image_paths=test_image_paths, is_test=True, transform=transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Initialize and train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepLabV3Model(device=device)\n",
        "model.train(train_dataloader, num_epochs=10)\n",
        "\n",
        "# Evaluate the model\n",
        "model.evaluate(test_dataloader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0AEWNbbOhQa",
        "outputId": "ee16a216-b2d1-40d5-cf1b-5cbd0733e2c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-7-a7c63ddc9f0e>:81: RuntimeWarning: divide by zero encountered in divide\n",
            "  return np.clip(g * (nir - red) / (nir + c1 * red - c2 * blue + l), 0, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q9nEjAahNU07"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}